{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genutzte Umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818070484, \"message\": \"system is installed with version sys.version_info(major=3, minor=8, micro=10, releaselevel='final', serial=0)\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070484, \"message\": \"pandas is installed with version 1.4.3\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070484, \"message\": \"numpy is installed with version 1.23.0\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from logger import Logger\n",
    "from test_executer import TestExecutor\n",
    "import extract\n",
    "import sys\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "logger = Logger()\n",
    "testExecutor = TestExecutor(logger)\n",
    "\n",
    "dependencies = [\n",
    "    ('system', sys.version_info),\n",
    "    ('pandas', pd.__version__),\n",
    "    ('numpy', np.__version__),\n",
    "]\n",
    "for dependency in dependencies:\n",
    "    logger.log(f\"{dependency[0]} is installed with version {dependency[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup der Daten\n",
    "\n",
    "Zuerst laden wir die ben√∂tigten Daten herunter und initialisieren die genutzten Python Objekte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\n",
    "    \"careplans\",\n",
    "    \"conditions\",\n",
    "    \"observations\",\n",
    "    \"patients\",\n",
    "]\n",
    "\n",
    "files = [\n",
    "    \"data/others/\",\n",
    "    \"data/asthma/\",\n",
    "    \"data/gallstones/\",\n",
    "    \"data/hypertension/\",\n",
    "]\n",
    "\n",
    "md5Hashes = {\n",
    "    \"data/others/careplans.csv\": \"365403e27541792755361bc0f6125506\",\n",
    "    \"data/others/conditions.csv\": \"ce0034e9ed9185b7d4c408ee9916de18\",\n",
    "    \"data/others/observations.csv\": \"b9e3bf1b033dc4af7f7ade78a48a50a4\",\n",
    "    \"data/others/patients.csv\": \"530570c8e30b77a822b37e927d1486b2\",\n",
    "    \"data/asthma/careplans.csv\": \"3c4aff1d0d576de6624c3726ec2dd544\",\n",
    "    \"data/asthma/conditions.csv\": \"e7965095ec41ef88498540341c79c49e\",\n",
    "    \"data/asthma/observations.csv\": \"1b8583de62d4d9e80c224005d74dd736\",\n",
    "    \"data/asthma/patients.csv\": \"b139ef00c850308c3d3f8e7fa0f97724\",\n",
    "    \"data/gallstones/careplans.csv\": \"35399fb01b2771b770c2f9f312e62dc2\",\n",
    "    \"data/gallstones/conditions.csv\": \"8a19bf13191cf074c64534c2fa01f15c\",\n",
    "    \"data/gallstones/observations.csv\": \"9d3807dc05cd7b4ccc3f0ee7b4f7b55e\",\n",
    "    \"data/gallstones/patients.csv\": \"3766f46941ee2155e0d1ed6e749e8ba7\",\n",
    "    \"data/hypertension/careplans.csv\": \"ddf053c4e56f24c4ce28e6d57edfd8b1\",\n",
    "    \"data/hypertension/conditions.csv\": \"8310cdc07924b48e07aa841f9075b488\",\n",
    "    \"data/hypertension/observations.csv\": \"f7564c732eebe9ace17a46e50b3cc857\",\n",
    "    \"data/hypertension/patients.csv\": \"2ebdf6b168e9c968ffa949463cd074e7\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/allergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818070713, \"message\": \"File data/others/careplans.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070714, \"message\": \"File data/others/conditions.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070715, \"message\": \"File data/others/observations.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070739, \"message\": \"File data/others/patients.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070739, \"message\": \"File data/asthma/careplans.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070740, \"message\": \"File data/asthma/conditions.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070742, \"message\": \"File data/asthma/observations.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070844, \"message\": \"File data/asthma/patients.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070846, \"message\": \"File data/gallstones/careplans.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070847, \"message\": \"File data/gallstones/conditions.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070849, \"message\": \"File data/gallstones/observations.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070921, \"message\": \"File data/gallstones/patients.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070924, \"message\": \"File data/hypertension/careplans.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070925, \"message\": \"File data/hypertension/conditions.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070927, \"message\": \"File data/hypertension/observations.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071017, \"message\": \"File data/hypertension/patients.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071020, \"message\": \"‚úÖ Using original data set\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "def ensure_file_has_been_downloaded(filename):\n",
    "    full_filename = \"../\" + filename\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/Fuenfgeld/DMA2022DataProjectC/main/\" + filename\n",
    "    if os.path.isfile(full_filename):\n",
    "        logger.log(\"File {} already exists, skipping download\".format(filename))\n",
    "    else:\n",
    "        logger.log(\"Downloading {}\".format(filename))\n",
    "        download_file(url, full_filename)\n",
    "\n",
    "def download_file(url, filename):\n",
    "    with open(filename, 'wb') as out_file:\n",
    "        with urlopen(url) as file:\n",
    "            out_file.write(file.read())\n",
    "\n",
    "if not os.path.isfile(\"extract.py\"):\n",
    "    download_file(\n",
    "        \"https://raw.githubusercontent.com/Fuenfgeld/DMA2022DataProjectC/main/src/extract.py\",\n",
    "        \"extract.py\"\n",
    "    )\n",
    "\n",
    "dataChanged = False\n",
    "for file in files:\n",
    "    for table in tables:\n",
    "        filename = file+table+\".csv\"\n",
    "        ensure_file_has_been_downloaded(filename)\n",
    "\n",
    "        with open(\"../\" + filename) as fileHandle:\n",
    "            fileContent = fileHandle.read()\n",
    "            fileHandle.close()\n",
    "\n",
    "        md5Hash = hashlib.md5(fileContent.encode()).hexdigest()\n",
    "        if md5Hashes[filename] != md5Hash:\n",
    "            dataChanged = True\n",
    "    \n",
    "if dataChanged:\n",
    "    logger.log(\"‚ùå Data set changed\")\n",
    "else:\n",
    "    logger.log(\"‚úÖ Using original data set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mit Datenbank verbinden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "databaseFile = \"data.sqlite\"\n",
    "\n",
    "logger.startTimeMeasurement('open-db', 'Connected to db and created tables')\n",
    "connection = extract.connect_to_db(logger, databaseFile)  # create table patients, observations, conditions, careplans\n",
    "logger.endTimeMeasurement('open-db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818071067, \"message\": \"‚úÖ Test ran successfully: Test connection to database\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "def test_sqliteConnection(_logger):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tablesInDb = list(map(lambda tableResult: tableResult[0], cursor.fetchall()))\n",
    "    tablesInDb.sort()\n",
    "\n",
    "    for table in tables:\n",
    "        if not(table in tablesInDb):\n",
    "            raise Exception('Table not found:', table)\n",
    "\n",
    "testExecutor.execute('Test connection to database', test_sqliteConnection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten in Datenbank laden\n",
    "\n",
    "Lade der verwendete Daten in die Datenbank:\n",
    "\n",
    "-   careplans\n",
    "-   conditions\n",
    "-   observations\n",
    "-   patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818071089, \"message\": \"üèó Extracting data from ../data/others/careplans.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071095, \"message\": \"üèó Extracting data from ../data/others/conditions.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071111, \"message\": \"üèó Extracting data from ../data/others/observations.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071415, \"message\": \"üèó Extracting data from ../data/others/patients.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071420, \"message\": \"üèó Extracting data from ../data/asthma/careplans.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071429, \"message\": \"üèó Extracting data from ../data/asthma/conditions.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071499, \"message\": \"üèó Extracting data from ../data/asthma/observations.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818072487, \"message\": \"üèó Extracting data from ../data/asthma/patients.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818072496, \"message\": \"üèó Extracting data from ../data/gallstones/careplans.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818072511, \"message\": \"üèó Extracting data from ../data/gallstones/conditions.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818072551, \"message\": \"üèó Extracting data from ../data/gallstones/observations.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818073512, \"message\": \"üèó Extracting data from ../data/gallstones/patients.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818073520, \"message\": \"üèó Extracting data from ../data/hypertension/careplans.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818073532, \"message\": \"üèó Extracting data from ../data/hypertension/conditions.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818073573, \"message\": \"üèó Extracting data from ../data/hypertension/observations.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818074703, \"message\": \"üèó Extracting data from ../data/hypertension/patients.csv\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "logger.startTimeMeasurement('load-data', 'Loading data into db')\n",
    "for file in files:\n",
    "    for table in tables:\n",
    "        extract.insert_values_to_table(logger, connection.cursor(), table, \"../\"+ file + table + \".csv\")  # TODO: insert ALL values in the right tables \n",
    "        connection.commit()\n",
    "\n",
    "logger.endTimeMeasurement('load-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annonymisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818075967, \"message\": \"‚úÖ Test ran successfully: Sanity check: extracting all ids worked\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818076001, \"message\": \"1330 unique patient ids found\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818076002, \"message\": \"‚ö†Ô∏è The dataset contains 1330 unique patientIds but only 1326 patients.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818076003, \"message\": \"‚úÖ Test ran successfully: Sanity check: no origin ids exist anymore\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "patientDf = pd.read_sql_query('SELECT * FROM patients;', connection)\n",
    "careplansDf = pd.read_sql_query('SELECT * FROM careplans;', connection)\n",
    "conditionsDf = pd.read_sql_query('SELECT * FROM conditions;', connection)\n",
    "observationsDf = pd.read_sql_query('SELECT * FROM observations;', connection)\n",
    "\n",
    "patientIds = [*patientDf.Id, *careplansDf.PATIENT, *conditionsDf.PATIENT, *observationsDf.PATIENT]\n",
    "\n",
    "def test_sanityCheckCombiningIds(_logger): \n",
    "    expectedLen = len(patientDf) + len(careplansDf) + len(conditionsDf) + len(observationsDf)\n",
    "    actualLen = len(patientIds)\n",
    "    if actualLen != expectedLen:\n",
    "        raise Exception('Not all patient ids were concatenated')\n",
    "\n",
    "testExecutor.execute('Sanity check: extracting all ids worked', test_sanityCheckCombiningIds)\n",
    "\n",
    "# Converts list to a set with only unique values\n",
    "uniqueIds = set(patientIds)\n",
    "logger.log(f\"{len(uniqueIds)} unique patient ids found\")\n",
    "if len(uniqueIds) >= len(patientDf.Id):\n",
    "    logger.log(f\"‚ö†Ô∏è The dataset contains {len(uniqueIds)} unique patientIds but only {len(patientDf.Id)} patients.\")\n",
    "\n",
    "annonymizedIds = {}\n",
    "for id in uniqueIds:\n",
    "    # Use uppercase here so it is easy to see if annonymized ids are used.\n",
    "    annonymizedIds[id] = hashlib.sha256(f\"{id}={random.random()}\".encode()).hexdigest().upper()\n",
    "\n",
    "def test_sanityEnsureAllIdsAreAnnonymized(_logger): \n",
    "    for id in patientIds:\n",
    "        if id in annonymizedIds:\n",
    "            raise Exception('A origin id still exists in anonnymized id list')\n",
    "\n",
    "testExecutor.execute('Sanity check: no origin ids exist anymore', test_sanityCheckCombiningIds)\n",
    "\n",
    "logger.startTimeMeasurement('annonymizedPatients', 'Writing annonymized patients')\n",
    "patientDf = patientDf.replace({\"Id\": annonymizedIds})\n",
    "patientDf.to_sql(name=\"anonnymized_patients\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('annonymizedPatients')\n",
    "\n",
    "logger.startTimeMeasurement('annonymizedCareplans', 'Writing annonymized careplans')\n",
    "careplansDf = careplansDf.replace({\"PATIENT\": annonymizedIds})\n",
    "careplansDf.to_sql(name=\"anonnymized_careplans\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('annonymizedCareplans')\n",
    "\n",
    "logger.startTimeMeasurement('annonymizedConditions', 'Writing annonymized conditions')\n",
    "conditionsDf = conditionsDf.replace({\"PATIENT\": annonymizedIds})\n",
    "conditionsDf.to_sql(name=\"anonnymized_conditions\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('annonymizedConditions')\n",
    "\n",
    "logger.startTimeMeasurement('annonymizedObservations', 'Writing annonymized Observations')\n",
    "observationsDf = observationsDf.replace({\"PATIENT\": annonymizedIds})\n",
    "observationsDf.to_sql(name=\"anonnymized_observations\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('annonymizedObservations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messung der Datenfehler\n",
    "\n",
    "F√ºr unsere Forschungsfrage sind nur alle Daten mit gemessenen BMI relevant. Wurde dieser nicht vermessen oder eingetragen k√∂nnen die Daten f√ºr die Forschungsfrage nicht verwendet werden und sind somit unbrauchbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL-Values\n",
    "\n",
    "Die Rohdaten werden zuvor auf die Anzahl an NULL-Values √ºberpr√ºft. Weisen mehr als **ein drittel der Daten**   L√ºcken in der Codierung auf, wird ein Fehler in der Verfassung angenommen und die Daten m√ºssen manuell √úberpr√ºft werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818129142, \"message\": \"Found Id                   0 NULL-Values in Column START                0 NULL-Values in Column STOP                 0 NULL-Values in Column PATIENT              0 NULL-Values in Column ENCOUNTER            0 NULL-Values in Column CODE                 0 NULL-Values in Column DESCRIPTION          0 NULL-Values in Column REASONCODE           0 NULL-Values in Column REASONDESCRIPTION    0 NULL-Values in Column dtype: int64 null-values in careplans.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818129212, \"message\": \"Found START          0 NULL-Values in Column STOP           0 NULL-Values in Column PATIENT        0 NULL-Values in Column ENCOUNTER      0 NULL-Values in Column CODE           0 NULL-Values in Column DESCRIPTION    0 NULL-Values in Column dtype: int64 null-values in conditions.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818130965, \"message\": \"Found DATE                0 NULL-Values in Column PATIENT             0 NULL-Values in Column ENCOUNTER           0 NULL-Values in Column OBSERVATION_TYPE    0 NULL-Values in Column CODE                0 NULL-Values in Column DESCRIPTION         0 NULL-Values in Column VALUE               0 NULL-Values in Column UNITS               0 NULL-Values in Column TYPE                0 NULL-Values in Column dtype: int64 null-values in observations.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818131770, \"message\": \"Found Id                     0 NULL-Values in Column BIRTHDATE              0 NULL-Values in Column DEATHDATE              0 NULL-Values in Column SSN                    0 NULL-Values in Column DRIVERS                0 NULL-Values in Column PASSPORT               0 NULL-Values in Column PREFIX                 0 NULL-Values in Column FIRST                  0 NULL-Values in Column LAST                   0 NULL-Values in Column SUFFIX                 0 NULL-Values in Column MAIDEN                 0 NULL-Values in Column MARITAL                0 NULL-Values in Column RACE                   0 NULL-Values in Column ETHNICITY              0 NULL-Values in Column GENDER                 0 NULL-Values in Column BIRTHPLACE             0 NULL-Values in Column ADDRESS                0 NULL-Values in Column CITY                   0 NULL-Values in Column STATE                  0 NULL-Values in Column COUNTRY                0 NULL-Values in Column ZIP                    0 NULL-Values in Column LAT                    0 NULL-Values in Column LON                    0 NULL-Values in Column HEALTHCARE_EXPENSES    0 NULL-Values in Column HEALTHCARE_COVERAGE    0 NULL-Values in Column dtype: int64 null-values in patients.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818131773, \"message\": \"Found 0.0 null-values.\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "null_counter = 0\n",
    "num_of_elements = 0\n",
    "for table in tables:\n",
    "    querie = f\"SELECT * from {table};\"\n",
    "    df = pd.read_sql_query(querie,connection)\n",
    "    result_string = str(df.isna().sum()).replace(\"\\n\",\" NULL-Values in Column \")\n",
    "    logger.log(f\"Found {result_string} null-values in {table}.\")\n",
    "    null_counter = df.isna().sum().sum() + null_counter\n",
    "    num_of_elements = num_of_elements + df.size\n",
    "perc_null_val = round(null_counter / num_of_elements,3)\n",
    "\n",
    "if perc_null_val > 0.33:\n",
    "    logger.log(f\"Found {perc_null_val} null-values.\",type='Warning')\n",
    "else:\n",
    "    logger.log(f\"Found {perc_null_val} null-values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr√ºfung auf Duplikate\n",
    "\n",
    "Duplikate verf√§lschen die  Ergebnisse des Anlyseteil durch Steigerung der Grundgesamheit mit gleichen Werten. Somit m√ºssen die Daten auf Duplikate in den einzelnen Files √ºberpr√ºft werden, um gleiche Messungen zu finden und gegebenfalls im ETL-Process zu entfernen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818131815, \"message\": \"Found 0 duplicate-values in careplans.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818131924, \"message\": \"Found 0 duplicate-values in conditions.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818134069, \"message\": \"Found 20608 duplicate-values in observations.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818134234, \"message\": \"Found 0 duplicate-values in patients.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818134234, \"message\": \"Found 0.003 duplicate-values.\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "num_of_duplicates = 0\n",
    "num_of_elements = 0\n",
    "for table in tables:\n",
    "    querie = f\"SELECT * from {table};\"\n",
    "    df = pd.read_sql_query(querie,connection)\n",
    "    duplicates = df.groupby(df.columns.tolist()).size().reset_index().\\\n",
    "    rename(columns={0:'records'})\n",
    "    curr_num_duplicate = (duplicates.records -1).sum() \n",
    "    num_of_duplicates = num_of_duplicates + curr_num_duplicate\n",
    "    logger.log(f\"Found {curr_num_duplicate} duplicate-values in {table}.\")\n",
    "    num_of_elements = num_of_elements + df.size\n",
    "perc_duplicates = round(num_of_duplicates / num_of_elements,3)\n",
    "logger.log(f\"Found {perc_duplicates} duplicate-values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prozentuales Anzahl von Gewichts und BMI Werten f√ºr Patieten\n",
    "\n",
    "F√ºr unsere Forschungsfrage sind BMI-Werte relevant und m√ºssen f√ºr den Patienten mindestes einmal codiert worden sein. Um die Forschungsfrage mit den zur vorliegenden Daten zu beanworten, sollten auch hier mindestens **ein drittel der Daten** mit einen BMI Codiert worden sein.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818134384, \"message\": \"Total num of patients 1326.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818134384, \"message\": \"Found 450 patients (33.9%) with 3539 BMI-values.\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "all_patients_query = \"\"\"\n",
    "SELECT COUNT(id) FROM patients;\"\"\"\n",
    "count_bmi_query = \"\"\"\n",
    "SELECT COUNT(distinct id) FROM patients JOIN observations on patients.id == observations.patient WHERE observations.Code = '59576-9'\"\"\"\n",
    "\n",
    "count_all_bmi_query = f\"\"\"\n",
    "SELECT COUNT(patient) FROM observations WHERE observations.Code = '59576-9'\"\"\"\n",
    "\n",
    "patient_all_count = connection.execute(all_patients_query).fetchall()[0][0]\n",
    "patient_bmi_count = connection.execute(count_bmi_query).fetchall()[0][0]\n",
    "bmi_count = connection.execute(count_all_bmi_query).fetchall()[0][0]\n",
    "ratio = round(patient_bmi_count/patient_all_count, 3) \n",
    "\n",
    "logger.log(f\"Total num of patients {patient_all_count}.\")\n",
    "\n",
    "if ratio > 0.33:\n",
    "    logger.log(f\"Found {patient_bmi_count} patients ({round(ratio*100,3)}%) with {bmi_count} BMI-values.\")\n",
    "else:\n",
    "    logger.log(f\"Found {patient_bmi_count} patients ({round(ratio*100,3)}%) with {bmi_count} BMI-values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen Dimensionstabellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle patients_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f36dd0c3650>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table patients_\n",
    "cursor.execute('''DROP TABLE IF EXISTS patients_;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE patients_ ( \n",
    "        ID STRING PRIMARY KEY UNIQUE,\n",
    "        RACE STRING, \n",
    "        ETHNICITY STRING,\n",
    "        GENDER STRING,\n",
    "        AGE INT64\n",
    "        );''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df patients\n",
    "cursor.execute('''SELECT ID, BIRTHDATE, DEATHDATE, RACE, ETHNICITY, GENDER FROM PATIENTS;''')\n",
    "df_patients = pd.DataFrame(cursor.fetchall(), columns=['ID', 'BIRTHDATE', 'DEATHDATE', 'RACE', 'ETHNICITY', 'GENDER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to date\n",
    "df_patients[\"DEATHDATE\"] = pd.to_datetime(df_patients[\"DEATHDATE\"])\n",
    "df_patients[\"BIRTHDATE\"] = pd.to_datetime(df_patients[\"BIRTHDATE\"])\n",
    "# fill null values withh todays date\n",
    "df_patients['DEATHDATE'] = df_patients.DEATHDATE.fillna(pd.to_datetime(\"today\"))\n",
    "# calculate age\n",
    "df_patients[\"AGE\"] = df_patients.DEATHDATE.dt.year - df_patients.BIRTHDATE.dt.year\n",
    "# drop unnecessary variables\n",
    "df_patients = df_patients.drop(['BIRTHDATE', 'DEATHDATE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1326 entries, 0 to 1325\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ID         1326 non-null   object\n",
      " 1   RACE       1326 non-null   object\n",
      " 2   ETHNICITY  1326 non-null   object\n",
      " 3   GENDER     1326 non-null   object\n",
      " 4   AGE        1326 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 51.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_patients.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Duplicated Rows 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Duplicated Rows\", df_patients.duplicated(df_patients.columns).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid       name    type  notnull dflt_value  pk\n",
      "0    0         ID  STRING        0       None   1\n",
      "1    1       RACE  STRING        0       None   0\n",
      "2    2  ETHNICITY  STRING        0       None   0\n",
      "3    3     GENDER  STRING        0       None   0\n",
      "4    4        AGE   INT64        0       None   0\n"
     ]
    }
   ],
   "source": [
    "df_patients.to_sql('df_patients', connection, if_exists='replace', index=False)\n",
    "cursor.execute('INSERT INTO patients_ (ID, RACE, ETHNICITY, GENDER, AGE) SELECT ID, RACE, ETHNICITY, GENDER, RACE FROM df_patients;')\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_patients;''')\n",
    "\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(patients_)\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle observations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid         name    type  notnull dflt_value  pk\n",
      "0    0         CODE  STRING        0       None   1\n",
      "1    1  DESCRIPTION  STRING        0       None   0\n",
      "2    2        UNITS  STRING        0       None   0\n",
      "3    3         TYPE  STRING        0       None   0\n"
     ]
    }
   ],
   "source": [
    "# table observations_\n",
    "cursor.execute('''DROP TABLE IF EXISTS observations_;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE observations_ ( \n",
    "        CODE STRING PRIMARY KEY UNIQUE,\n",
    "        DESCRIPTION STRING,\n",
    "        UNITS STRING, \n",
    "        TYPE STRING\n",
    "        );''')\n",
    "\n",
    "# create df observation\n",
    "cursor.execute('''SELECT CODE, DESCRIPTION, UNITS, TYPE FROM OBSERVATIONS;''')\n",
    "df_observations = pd.DataFrame(cursor.fetchall(), columns=['CODE','DESCRIPTION', 'UNITS', 'TYPE'])\n",
    "\n",
    "df_observations = df_observations.drop_duplicates(subset='CODE')\n",
    "\n",
    "# transform dt in table\n",
    "df_observations.to_sql('df_observations', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('''INSERT INTO observations_ (CODE, DESCRIPTION, UNITS, TYPE) SELECT CODE, DESCRIPTION, UNITS, TYPE FROM df_observations;''')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_observations;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(observations_)\", connection))\n",
    "#print(pd.read_sql_query(\"SELECT * FROM observations_\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle careplans_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid         name    type  notnull dflt_value  pk\n",
      "0    0         CODE  STRING        0       None   1\n",
      "1    1  DESCRIPTION  STRING        0       None   0\n"
     ]
    }
   ],
   "source": [
    "# table careplans_code\n",
    "cursor.execute('''DROP TABLE IF EXISTS careplans_code;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE careplans_code ( \n",
    "        CODE STRING PRIMARY KEY UNIQUE,\n",
    "        DESCRIPTION STRING\n",
    "        );''')\n",
    "\n",
    "# create df careplans_code\n",
    "cursor.execute('''SELECT CODE, DESCRIPTION FROM CAREPLANS;''')\n",
    "df_careplans_code = pd.DataFrame(cursor.fetchall(), columns=['CODE','DESCRIPTION'])\n",
    "\n",
    "df_careplans_code = df_careplans_code.drop_duplicates(subset='CODE')\n",
    "\n",
    "# transform dt in table\n",
    "df_careplans_code.to_sql('df_careplans_code', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('''INSERT INTO careplans_code (CODE, DESCRIPTION) SELECT CODE, DESCRIPTION FROM df_careplans_code;''')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_careplans_code;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(careplans_code)\", connection))\n",
    "#print(pd.read_sql_query(\"SELECT * FROM careplans_code\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle careplans_reasoncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid               name    type  notnull dflt_value  pk\n",
      "0    0         REASONCODE  STRING        0       None   1\n",
      "1    1  REASONDESCRIPTION  STRING        0       None   0\n"
     ]
    }
   ],
   "source": [
    "# table careplans_reasoncode\n",
    "cursor.execute('''DROP TABLE IF EXISTS careplans_reasoncode;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE careplans_reasoncode ( \n",
    "        REASONCODE STRING PRIMARY KEY UNIQUE,\n",
    "        REASONDESCRIPTION STRING\n",
    "        );''')\n",
    "\n",
    "# create df careplans_code\n",
    "cursor.execute('''SELECT REASONCODE, REASONDESCRIPTION FROM CAREPLANS;''')\n",
    "df_careplans_reasoncode = pd.DataFrame(cursor.fetchall(), columns=['REASONCODE','REASONDESCRIPTION'])\n",
    "\n",
    "df_careplans_reasoncode = df_careplans_reasoncode.drop_duplicates(subset='REASONCODE')\n",
    "\n",
    "# transform dt in table\n",
    "df_careplans_reasoncode.to_sql('df_careplans_reasoncode', connection, if_exists='replace', index=False)\n",
    "   \n",
    "cursor.execute('INSERT INTO careplans_reasoncode (REASONCODE, REASONDESCRIPTION) SELECT REASONCODE, REASONDESCRIPTION FROM df_careplans_reasoncode;')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_careplans_reasoncode;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(careplans_reasoncode)\", connection))\n",
    "#print(pd.read_sql_query(\"SELECT * FROM careplans_reasoncode\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle conditions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid         name    type  notnull dflt_value  pk\n",
      "0    0         CODE  STRING        0       None   1\n",
      "1    1  DESCRIPTION  STRING        0       None   0\n"
     ]
    }
   ],
   "source": [
    "# table conditions\n",
    "cursor.execute('''DROP TABLE IF EXISTS conditions_;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE conditions_ ( \n",
    "        CODE STRING PRIMARY KEY UNIQUE,\n",
    "        DESCRIPTION STRING\n",
    "        );''')\n",
    "\n",
    "# create df conditions\n",
    "cursor.execute('''SELECT CODE, DESCRIPTION FROM CONDITIONS;''')\n",
    "df_conditions = pd.DataFrame(cursor.fetchall(), columns=['CODE','DESCRIPTION'])\n",
    "\n",
    "df_conditions = df_conditions.drop_duplicates(subset='CODE')\n",
    "\n",
    "# transform dt in table\n",
    "df_conditions.to_sql('df_conditions', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('INSERT INTO conditions_ (CODE, DESCRIPTION) SELECT CODE, DESCRIPTION FROM df_conditions;')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_conditions;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(conditions_)\", connection))\n",
    "#print(pd.read_sql_query(\"SELECT * FROM conditions_\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datumstabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def create_date_table(start='1900-01-01', end=datetime.today().strftime('%Y-%m-%d')):\n",
    "    \n",
    "    df_date = pd.DataFrame({\"Date\": pd.date_range(start, end)})\n",
    "\n",
    "    days_names = {\n",
    "        i: name\n",
    "        for i, name\n",
    "        in enumerate(['Monday', 'Tuesday', 'Wednesday',\n",
    "                      'Thursday', 'Friday', 'Saturday', \n",
    "                      'Sunday'])\n",
    "    }\n",
    "   \n",
    "    df_date[\"Day\"] = df_date.Date.dt.dayofweek.map(days_names.get)\n",
    "    df_date[\"Week\"] = df_date.Date.dt.weekofyear\n",
    "    df_date[\"Quarter\"] = df_date.Date.dt.quarter\n",
    "    df_date[\"Year\"] = df_date.Date.dt.year\n",
    "    df_date[\"Year_half\"] = (df_date.Quarter + 1) // 2\n",
    "    \n",
    "    return df_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f36dd0c3650>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table date_table\n",
    "cursor.execute('''DROP TABLE IF EXISTS date_table;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE date_table ( \n",
    "        DATE DATE PRIMARY KEY UNIQUE,\n",
    "        DAY STRING,\n",
    "        WEEK INT16,\n",
    "        QUARTER INT16,\n",
    "        YEAR INT16,\n",
    "        YEAR_HALF INT16\n",
    "        );''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid       name    type  notnull dflt_value  pk\n",
      "0    0       DATE    DATE        0       None   1\n",
      "1    1        DAY  STRING        0       None   0\n",
      "2    2       WEEK   INT16        0       None   0\n",
      "3    3    QUARTER   INT16        0       None   0\n",
      "4    4       YEAR   INT16        0       None   0\n",
      "5    5  YEAR_HALF   INT16        0       None   0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113338/3083742411.py:15: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  df_date[\"Week\"] = df_date.Date.dt.weekofyear\n"
     ]
    }
   ],
   "source": [
    "df_date = create_date_table()\n",
    "\n",
    "# transform dt in table\n",
    "df_date.to_sql('df_date', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('INSERT INTO date_table (DATE, DAY, WEEK, QUARTER, YEAR, YEAR_HALF) SELECT DATE, DAY, WEEK, QUARTER, YEAR, YEAR_HALF FROM df_date;')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_date;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(date_table)\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstelle Faktentabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('''DROP TABLE IF EXISTS fact_table;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE fact_table ( \n",
    "        PATIENT_ID STRING,\n",
    "        OBSERVATION_CODE STRING,\n",
    "        VALUE STRING,\n",
    "        DATE DATE,\n",
    "        CONDITIONS_CODE STRING,\n",
    "        ENDDATE DATE,\n",
    "        CAREPLANS_CODE STRING,\n",
    "        CAREPLANS_REASONCODE STRING,\n",
    "        FOREIGN KEY (PATIENT_ID)\n",
    "            REFERENCES patients_(ID),\n",
    "        FOREIGN KEY (OBSERVATION_CODE)\n",
    "            REFERENCES observations_(CODE),\n",
    "        FOREIGN KEY (CONDITIONS_CODE)\n",
    "            REFERENCES conditions_(CODE),\n",
    "        FOREIGN KEY (CAREPLANS_CODE)\n",
    "            REFERENCES careplans_code(CODE),\n",
    "        FOREIGN KEY (CAREPLANS_REASONCODE)\n",
    "            REFERENCES careplans_reasoncode(REASONCODE)\n",
    "        );''')\n",
    "\n",
    "cursor.execute('''INSERT INTO fact_table        \n",
    "                    (PATIENT_ID, OBSERVATION_CODE, VALUE, DATE) \n",
    "                    SELECT PATIENT, CODE, VALUE, DATE \n",
    "                    FROM OBSERVATIONS\n",
    "                    ;''')\n",
    "\n",
    "cursor.execute('''INSERT INTO fact_table        \n",
    "                    (PATIENT_ID, CAREPLANS_CODE, DATE, ENDDATE) \n",
    "                    SELECT PATIENT, CODE, START, STOP \n",
    "                    FROM CAREPLANS\n",
    "                    ;''')\n",
    "\n",
    "cursor.execute('''INSERT INTO fact_table        \n",
    "                    (PATIENT_ID, CAREPLANS_REASONCODE, DATE, ENDDATE) \n",
    "                    SELECT PATIENT, REASONCODE, START, STOP \n",
    "                    FROM CAREPLANS\n",
    "                    ;''')\n",
    "\n",
    "cursor.execute('''INSERT INTO fact_table        \n",
    "                    (PATIENT_ID, CONDITIONS_CODE, DATE, ENDDATE) \n",
    "                    SELECT PATIENT, CODE, START, STOP \n",
    "                    FROM CONDITIONS\n",
    "                    ;''')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  seq                 table                  from          to  on_update  \\\n",
      "0   0    0  careplans_reasoncode  CAREPLANS_REASONCODE  REASONCODE  NO ACTION   \n",
      "1   1    0        careplans_code        CAREPLANS_CODE        CODE  NO ACTION   \n",
      "2   2    0           conditions_       CONDITIONS_CODE        CODE  NO ACTION   \n",
      "3   3    0         observations_      OBSERVATION_CODE        CODE  NO ACTION   \n",
      "4   4    0             patients_            PATIENT_ID          ID  NO ACTION   \n",
      "\n",
      "   on_delete match  \n",
      "0  NO ACTION  NONE  \n",
      "1  NO ACTION  NONE  \n",
      "2  NO ACTION  NONE  \n",
      "3  NO ACTION  NONE  \n",
      "4  NO ACTION  NONE  \n"
     ]
    }
   ],
   "source": [
    "print(pd.read_sql_query(\"PRAGMA foreign_key_list(fact_table)\", connection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('patients',), ('careplans',), ('conditions',), ('observations',), ('anonnymized_patients',), ('anonnymized_careplans',), ('anonnymized_conditions',), ('anonnymized_observations',), ('patients_',), ('observations_',), ('careplans_code',), ('careplans_reasoncode',), ('conditions_',), ('date_table',), ('fact_table',)]\n"
     ]
    }
   ],
   "source": [
    "cursor.execute('SELECT name FROM sqlite_master where type=\"table\"')\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufr√§umen & Logs speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818136086, \"message\": \"‚è≥ Connected to db and created tables in 1853261ms\", \"params\": {\"timingInMilliseconds\": 1853261}}\n",
      "{\"type\": \"info\", \"time\": 1657818136087, \"message\": \"‚è≥ Loading data into db in 1856849ms\", \"params\": {\"timingInMilliseconds\": 1856849}}\n",
      "{\"type\": \"info\", \"time\": 1657818136087, \"message\": \"‚è≥ Writing annonymized patients in 1852616ms\", \"params\": {\"timingInMilliseconds\": 1852616}}\n",
      "{\"type\": \"info\", \"time\": 1657818136087, \"message\": \"‚è≥ Writing annonymized careplans in 1852679ms\", \"params\": {\"timingInMilliseconds\": 1852679}}\n",
      "{\"type\": \"info\", \"time\": 1657818136087, \"message\": \"‚è≥ Writing annonymized conditions in 1853792ms\", \"params\": {\"timingInMilliseconds\": 1853792}}\n",
      "{\"type\": \"info\", \"time\": 1657818136087, \"message\": \"‚è≥ Writing annonymized Observations in 1904065ms\", \"params\": {\"timingInMilliseconds\": 1904065}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lukas/git/DMA2022DataProjectC/src/ETL.ipynb Cell 50'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lukas/git/DMA2022DataProjectC/src/ETL.ipynb#ch0000068?line=0'>1</a>\u001b[0m connection\u001b[39m.\u001b[39mclose()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lukas/git/DMA2022DataProjectC/src/ETL.ipynb#ch0000068?line=1'>2</a>\u001b[0m logger\u001b[39m.\u001b[39;49mlogTimings()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lukas/git/DMA2022DataProjectC/src/ETL.ipynb#ch0000068?line=2'>3</a>\u001b[0m logger\u001b[39m.\u001b[39mwriteToFile(\u001b[39m\"\u001b[39m\u001b[39m../artefacts-for-release/etl-log.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/DMA2022DataProjectC/src/logger.py:39\u001b[0m, in \u001b[0;36mLogger.logTimings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogTimings\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     38\u001b[0m     \u001b[39mfor\u001b[39;00m timing \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimings:\n\u001b[0;32m---> 39\u001b[0m         usedTime \u001b[39m=\u001b[39m timing[\u001b[39m'\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m-\u001b[39;49m timing[\u001b[39m'\u001b[39;49m\u001b[39mstart\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     40\u001b[0m         message \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m‚è≥ \u001b[39m\u001b[39m{\u001b[39;00mtiming[\u001b[39m'\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{\u001b[39;00musedTime\u001b[39m}\u001b[39;00m\u001b[39mms\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogWithTiming(message, usedTime)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "connection.close()\n",
    "logger.logTimings()\n",
    "logger.writeToFile(\"../artefacts-for-release/etl-log.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
