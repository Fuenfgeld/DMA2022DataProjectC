{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genutzte Umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818070484, \"message\": \"system is installed with version sys.version_info(major=3, minor=8, micro=10, releaselevel='final', serial=0)\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070484, \"message\": \"pandas is installed with version 1.4.3\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070484, \"message\": \"numpy is installed with version 1.23.0\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from logger import Logger\n",
    "from test_executer import TestExecutor\n",
    "import extract\n",
    "import sys\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "logger = Logger()\n",
    "testExecutor = TestExecutor(logger)\n",
    "\n",
    "dependencies = [\n",
    "    ('system', sys.version_info),\n",
    "    ('pandas', pd.__version__),\n",
    "    ('numpy', np.__version__),\n",
    "]\n",
    "for dependency in dependencies:\n",
    "    logger.log(f\"{dependency[0]} is installed with version {dependency[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup der Daten\n",
    "\n",
    "Zuerst laden wir die benötigten Daten herunter und initialisieren die genutzten Python Objekte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\n",
    "    \"careplans\",\n",
    "    \"conditions\",\n",
    "    \"observations\",\n",
    "    \"patients\",\n",
    "]\n",
    "\n",
    "files = [\n",
    "    \"data/others/\",\n",
    "    \"data/asthma/\",\n",
    "    \"data/gallstones/\",\n",
    "    \"data/hypertension/\",\n",
    "]\n",
    "\n",
    "md5Hashes = {\n",
    "    \"data/others/careplans.csv\": \"365403e27541792755361bc0f6125506\",\n",
    "    \"data/others/conditions.csv\": \"ce0034e9ed9185b7d4c408ee9916de18\",\n",
    "    \"data/others/observations.csv\": \"b9e3bf1b033dc4af7f7ade78a48a50a4\",\n",
    "    \"data/others/patients.csv\": \"530570c8e30b77a822b37e927d1486b2\",\n",
    "    \"data/asthma/careplans.csv\": \"3c4aff1d0d576de6624c3726ec2dd544\",\n",
    "    \"data/asthma/conditions.csv\": \"e7965095ec41ef88498540341c79c49e\",\n",
    "    \"data/asthma/observations.csv\": \"1b8583de62d4d9e80c224005d74dd736\",\n",
    "    \"data/asthma/patients.csv\": \"b139ef00c850308c3d3f8e7fa0f97724\",\n",
    "    \"data/gallstones/careplans.csv\": \"35399fb01b2771b770c2f9f312e62dc2\",\n",
    "    \"data/gallstones/conditions.csv\": \"8a19bf13191cf074c64534c2fa01f15c\",\n",
    "    \"data/gallstones/observations.csv\": \"9d3807dc05cd7b4ccc3f0ee7b4f7b55e\",\n",
    "    \"data/gallstones/patients.csv\": \"3766f46941ee2155e0d1ed6e749e8ba7\",\n",
    "    \"data/hypertension/careplans.csv\": \"ddf053c4e56f24c4ce28e6d57edfd8b1\",\n",
    "    \"data/hypertension/conditions.csv\": \"8310cdc07924b48e07aa841f9075b488\",\n",
    "    \"data/hypertension/observations.csv\": \"f7564c732eebe9ace17a46e50b3cc857\",\n",
    "    \"data/hypertension/patients.csv\": \"2ebdf6b168e9c968ffa949463cd074e7\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/allergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818070713, \"message\": \"File data/others/careplans.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070714, \"message\": \"File data/others/conditions.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070715, \"message\": \"File data/others/observations.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070739, \"message\": \"File data/others/patients.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070739, \"message\": \"File data/asthma/careplans.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070740, \"message\": \"File data/asthma/conditions.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070742, \"message\": \"File data/asthma/observations.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070844, \"message\": \"File data/asthma/patients.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070846, \"message\": \"File data/gallstones/careplans.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070847, \"message\": \"File data/gallstones/conditions.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070849, \"message\": \"File data/gallstones/observations.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070921, \"message\": \"File data/gallstones/patients.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070924, \"message\": \"File data/hypertension/careplans.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070925, \"message\": \"File data/hypertension/conditions.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818070927, \"message\": \"File data/hypertension/observations.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071017, \"message\": \"File data/hypertension/patients.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071020, \"message\": \"✅ Using original data set\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "def ensure_file_has_been_downloaded(filename):\n",
    "    full_filename = \"../\" + filename\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/Fuenfgeld/DMA2022DataProjectC/main/\" + filename\n",
    "    if os.path.isfile(full_filename):\n",
    "        logger.log(\"File {} already exists, skipping download\".format(filename))\n",
    "    else:\n",
    "        logger.log(\"Downloading {}\".format(filename))\n",
    "        download_file(url, full_filename)\n",
    "\n",
    "def download_file(url, filename):\n",
    "    with open(filename, 'wb') as out_file:\n",
    "        with urlopen(url) as file:\n",
    "            out_file.write(file.read())\n",
    "\n",
    "if not os.path.isfile(\"extract.py\"):\n",
    "    download_file(\n",
    "        \"https://raw.githubusercontent.com/Fuenfgeld/DMA2022DataProjectC/main/src/extract.py\",\n",
    "        \"extract.py\"\n",
    "    )\n",
    "\n",
    "dataChanged = False\n",
    "for file in files:\n",
    "    for table in tables:\n",
    "        filename = file+table+\".csv\"\n",
    "        ensure_file_has_been_downloaded(filename)\n",
    "\n",
    "        with open(\"../\" + filename) as fileHandle:\n",
    "            fileContent = fileHandle.read()\n",
    "            fileHandle.close()\n",
    "\n",
    "        md5Hash = hashlib.md5(fileContent.encode()).hexdigest()\n",
    "        if md5Hashes[filename] != md5Hash:\n",
    "            dataChanged = True\n",
    "    \n",
    "if dataChanged:\n",
    "    logger.log(\"❌ Data set changed\")\n",
    "else:\n",
    "    logger.log(\"✅ Using original data set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mit Datenbank verbinden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "databaseFile = \"data.sqlite\"\n",
    "\n",
    "logger.startTimeMeasurement('open-db', 'Connected to db and created tables')\n",
    "connection = extract.connect_to_db(logger, databaseFile)  # create table patients, observations, conditions, careplans\n",
    "logger.endTimeMeasurement('open-db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818071067, \"message\": \"✅ Test ran successfully: Test connection to database\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "def test_sqliteConnection(_logger):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tablesInDb = list(map(lambda tableResult: tableResult[0], cursor.fetchall()))\n",
    "    tablesInDb.sort()\n",
    "\n",
    "    for table in tables:\n",
    "        if not(table in tablesInDb):\n",
    "            raise Exception('Table not found:', table)\n",
    "\n",
    "testExecutor.execute('Test connection to database', test_sqliteConnection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten in Datenbank laden\n",
    "\n",
    "Lade der verwendete Daten in die Datenbank:\n",
    "\n",
    "-   careplans\n",
    "-   conditions\n",
    "-   observations\n",
    "-   patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818071089, \"message\": \"🏗 Extracting data from ../data/others/careplans.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071095, \"message\": \"🏗 Extracting data from ../data/others/conditions.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071111, \"message\": \"🏗 Extracting data from ../data/others/observations.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071415, \"message\": \"🏗 Extracting data from ../data/others/patients.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071420, \"message\": \"🏗 Extracting data from ../data/asthma/careplans.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071429, \"message\": \"🏗 Extracting data from ../data/asthma/conditions.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818071499, \"message\": \"🏗 Extracting data from ../data/asthma/observations.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818072487, \"message\": \"🏗 Extracting data from ../data/asthma/patients.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818072496, \"message\": \"🏗 Extracting data from ../data/gallstones/careplans.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818072511, \"message\": \"🏗 Extracting data from ../data/gallstones/conditions.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818072551, \"message\": \"🏗 Extracting data from ../data/gallstones/observations.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818073512, \"message\": \"🏗 Extracting data from ../data/gallstones/patients.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818073520, \"message\": \"🏗 Extracting data from ../data/hypertension/careplans.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818073532, \"message\": \"🏗 Extracting data from ../data/hypertension/conditions.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818073573, \"message\": \"🏗 Extracting data from ../data/hypertension/observations.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818074703, \"message\": \"🏗 Extracting data from ../data/hypertension/patients.csv\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "logger.startTimeMeasurement('load-data', 'Loading data into db')\n",
    "for file in files:\n",
    "    for table in tables:\n",
    "        extract.insert_values_to_table(logger, connection.cursor(), table, \"../\"+ file + table + \".csv\")  # TODO: insert ALL values in the right tables \n",
    "        connection.commit()\n",
    "\n",
    "logger.endTimeMeasurement('load-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annonymisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818075967, \"message\": \"✅ Test ran successfully: Sanity check: extracting all ids worked\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818076001, \"message\": \"1330 unique patient ids found\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818076002, \"message\": \"⚠️ The dataset contains 1330 unique patientIds but only 1326 patients.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818076003, \"message\": \"✅ Test ran successfully: Sanity check: no origin ids exist anymore\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "patientDf = pd.read_sql_query('SELECT * FROM patients;', connection)\n",
    "careplansDf = pd.read_sql_query('SELECT * FROM careplans;', connection)\n",
    "conditionsDf = pd.read_sql_query('SELECT * FROM conditions;', connection)\n",
    "observationsDf = pd.read_sql_query('SELECT * FROM observations;', connection)\n",
    "\n",
    "patientIds = [*patientDf.Id, *careplansDf.PATIENT, *conditionsDf.PATIENT, *observationsDf.PATIENT]\n",
    "\n",
    "def test_sanityCheckCombiningIds(_logger): \n",
    "    expectedLen = len(patientDf) + len(careplansDf) + len(conditionsDf) + len(observationsDf)\n",
    "    actualLen = len(patientIds)\n",
    "    if actualLen != expectedLen:\n",
    "        raise Exception('Not all patient ids were concatenated')\n",
    "\n",
    "testExecutor.execute('Sanity check: extracting all ids worked', test_sanityCheckCombiningIds)\n",
    "\n",
    "# Converts list to a set with only unique values\n",
    "uniqueIds = set(patientIds)\n",
    "logger.log(f\"{len(uniqueIds)} unique patient ids found\")\n",
    "if len(uniqueIds) >= len(patientDf.Id):\n",
    "    logger.log(f\"⚠️ The dataset contains {len(uniqueIds)} unique patientIds but only {len(patientDf.Id)} patients.\")\n",
    "\n",
    "annonymizedIds = {}\n",
    "for id in uniqueIds:\n",
    "    # Use uppercase here so it is easy to see if annonymized ids are used.\n",
    "    annonymizedIds[id] = hashlib.sha256(f\"{id}={random.random()}\".encode()).hexdigest().upper()\n",
    "\n",
    "def test_sanityEnsureAllIdsAreAnnonymized(_logger): \n",
    "    for id in patientIds:\n",
    "        if id in annonymizedIds:\n",
    "            raise Exception('A origin id still exists in anonnymized id list')\n",
    "\n",
    "testExecutor.execute('Sanity check: no origin ids exist anymore', test_sanityCheckCombiningIds)\n",
    "\n",
    "logger.startTimeMeasurement('annonymizedPatients', 'Writing annonymized patients')\n",
    "patientDf = patientDf.replace({\"Id\": annonymizedIds})\n",
    "patientDf.to_sql(name=\"anonnymized_patients\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('annonymizedPatients')\n",
    "\n",
    "logger.startTimeMeasurement('annonymizedCareplans', 'Writing annonymized careplans')\n",
    "careplansDf = careplansDf.replace({\"PATIENT\": annonymizedIds})\n",
    "careplansDf.to_sql(name=\"anonnymized_careplans\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('annonymizedCareplans')\n",
    "\n",
    "logger.startTimeMeasurement('annonymizedConditions', 'Writing annonymized conditions')\n",
    "conditionsDf = conditionsDf.replace({\"PATIENT\": annonymizedIds})\n",
    "conditionsDf.to_sql(name=\"anonnymized_conditions\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('annonymizedConditions')\n",
    "\n",
    "logger.startTimeMeasurement('annonymizedObservations', 'Writing annonymized Observations')\n",
    "observationsDf = observationsDf.replace({\"PATIENT\": annonymizedIds})\n",
    "observationsDf.to_sql(name=\"anonnymized_observations\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('annonymizedObservations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messung der Datenfehler\n",
    "\n",
    "Für unsere Forschungsfrage sind nur alle Daten mit gemessenen BMI relevant. Wurde dieser nicht vermessen oder eingetragen können die Daten für die Forschungsfrage nicht verwendet werden und sind somit unbrauchbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL-Values\n",
    "\n",
    "Die Rohdaten werden zuvor auf die Anzahl an NULL-Values überprüft. Weisen mehr als **ein drittel der Daten**   Lücken in der Codierung auf, wird ein Fehler in der Verfassung angenommen und die Daten müssen manuell Überprüft werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818129142, \"message\": \"Found Id                   0 NULL-Values in Column START                0 NULL-Values in Column STOP                 0 NULL-Values in Column PATIENT              0 NULL-Values in Column ENCOUNTER            0 NULL-Values in Column CODE                 0 NULL-Values in Column DESCRIPTION          0 NULL-Values in Column REASONCODE           0 NULL-Values in Column REASONDESCRIPTION    0 NULL-Values in Column dtype: int64 null-values in careplans.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818129212, \"message\": \"Found START          0 NULL-Values in Column STOP           0 NULL-Values in Column PATIENT        0 NULL-Values in Column ENCOUNTER      0 NULL-Values in Column CODE           0 NULL-Values in Column DESCRIPTION    0 NULL-Values in Column dtype: int64 null-values in conditions.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818130965, \"message\": \"Found DATE                0 NULL-Values in Column PATIENT             0 NULL-Values in Column ENCOUNTER           0 NULL-Values in Column OBSERVATION_TYPE    0 NULL-Values in Column CODE                0 NULL-Values in Column DESCRIPTION         0 NULL-Values in Column VALUE               0 NULL-Values in Column UNITS               0 NULL-Values in Column TYPE                0 NULL-Values in Column dtype: int64 null-values in observations.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818131770, \"message\": \"Found Id                     0 NULL-Values in Column BIRTHDATE              0 NULL-Values in Column DEATHDATE              0 NULL-Values in Column SSN                    0 NULL-Values in Column DRIVERS                0 NULL-Values in Column PASSPORT               0 NULL-Values in Column PREFIX                 0 NULL-Values in Column FIRST                  0 NULL-Values in Column LAST                   0 NULL-Values in Column SUFFIX                 0 NULL-Values in Column MAIDEN                 0 NULL-Values in Column MARITAL                0 NULL-Values in Column RACE                   0 NULL-Values in Column ETHNICITY              0 NULL-Values in Column GENDER                 0 NULL-Values in Column BIRTHPLACE             0 NULL-Values in Column ADDRESS                0 NULL-Values in Column CITY                   0 NULL-Values in Column STATE                  0 NULL-Values in Column COUNTRY                0 NULL-Values in Column ZIP                    0 NULL-Values in Column LAT                    0 NULL-Values in Column LON                    0 NULL-Values in Column HEALTHCARE_EXPENSES    0 NULL-Values in Column HEALTHCARE_COVERAGE    0 NULL-Values in Column dtype: int64 null-values in patients.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818131773, \"message\": \"Found 0.0 null-values.\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "null_counter = 0\n",
    "num_of_elements = 0\n",
    "for table in tables:\n",
    "    querie = f\"SELECT * from {table};\"\n",
    "    df = pd.read_sql_query(querie,connection)\n",
    "    result_string = str(df.isna().sum()).replace(\"\\n\",\" NULL-Values in Column \")\n",
    "    logger.log(f\"Found {result_string} null-values in {table}.\")\n",
    "    null_counter = df.isna().sum().sum() + null_counter\n",
    "    num_of_elements = num_of_elements + df.size\n",
    "perc_null_val = round(null_counter / num_of_elements,3)\n",
    "\n",
    "if perc_null_val > 0.33:\n",
    "    logger.log(f\"Found {perc_null_val} null-values.\",type='Warning')\n",
    "else:\n",
    "    logger.log(f\"Found {perc_null_val} null-values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prüfung auf Duplikate\n",
    "\n",
    "Duplikate verfälschen die  Ergebnisse des Anlyseteil durch Steigerung der Grundgesamheit mit gleichen Werten. Somit müssen die Daten auf Duplikate in den einzelnen Files überprüft werden, um gleiche Messungen zu finden und gegebenfalls im ETL-Process zu entfernen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818131815, \"message\": \"Found 0 duplicate-values in careplans.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818131924, \"message\": \"Found 0 duplicate-values in conditions.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818134069, \"message\": \"Found 20608 duplicate-values in observations.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818134234, \"message\": \"Found 0 duplicate-values in patients.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818134234, \"message\": \"Found 0.003 duplicate-values.\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "num_of_duplicates = 0\n",
    "num_of_elements = 0\n",
    "for table in tables:\n",
    "    querie = f\"SELECT * from {table};\"\n",
    "    df = pd.read_sql_query(querie,connection)\n",
    "    duplicates = df.groupby(df.columns.tolist()).size().reset_index().\\\n",
    "    rename(columns={0:'records'})\n",
    "    curr_num_duplicate = (duplicates.records -1).sum() \n",
    "    num_of_duplicates = num_of_duplicates + curr_num_duplicate\n",
    "    logger.log(f\"Found {curr_num_duplicate} duplicate-values in {table}.\")\n",
    "    num_of_elements = num_of_elements + df.size\n",
    "perc_duplicates = round(num_of_duplicates / num_of_elements,3)\n",
    "logger.log(f\"Found {perc_duplicates} duplicate-values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prozentuales Anzahl von Gewichts und BMI Werten für Patieten\n",
    "\n",
    "Für unsere Forschungsfrage sind BMI-Werte relevant und müssen für den Patienten mindestes einmal codiert worden sein. Um die Forschungsfrage mit den zur vorliegenden Daten zu beanworten, sollten auch hier mindestens **ein drittel der Daten** mit einen BMI Codiert worden sein.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818134384, \"message\": \"Total num of patients 1326.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1657818134384, \"message\": \"Found 450 patients (33.9%) with 3539 BMI-values.\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "all_patients_query = \"\"\"\n",
    "SELECT COUNT(id) FROM patients;\"\"\"\n",
    "count_bmi_query = \"\"\"\n",
    "SELECT COUNT(distinct id) FROM patients JOIN observations on patients.id == observations.patient WHERE observations.Code = '59576-9'\"\"\"\n",
    "\n",
    "count_all_bmi_query = f\"\"\"\n",
    "SELECT COUNT(patient) FROM observations WHERE observations.Code = '59576-9'\"\"\"\n",
    "\n",
    "patient_all_count = connection.execute(all_patients_query).fetchall()[0][0]\n",
    "patient_bmi_count = connection.execute(count_bmi_query).fetchall()[0][0]\n",
    "bmi_count = connection.execute(count_all_bmi_query).fetchall()[0][0]\n",
    "ratio = round(patient_bmi_count/patient_all_count, 3) \n",
    "\n",
    "logger.log(f\"Total num of patients {patient_all_count}.\")\n",
    "\n",
    "if ratio > 0.33:\n",
    "    logger.log(f\"Found {patient_bmi_count} patients ({round(ratio*100,3)}%) with {bmi_count} BMI-values.\")\n",
    "else:\n",
    "    logger.log(f\"Found {patient_bmi_count} patients ({round(ratio*100,3)}%) with {bmi_count} BMI-values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen Dimensionstabellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle patients_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f36dd0c3650>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table patients_\n",
    "cursor.execute('''DROP TABLE IF EXISTS patients_;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE patients_ ( \n",
    "        ID STRING PRIMARY KEY UNIQUE,\n",
    "        RACE STRING, \n",
    "        ETHNICITY STRING,\n",
    "        GENDER STRING,\n",
    "        AGE INT64\n",
    "        );''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df patients\n",
    "cursor.execute('''SELECT ID, BIRTHDATE, DEATHDATE, RACE, ETHNICITY, GENDER FROM PATIENTS;''')\n",
    "df_patients = pd.DataFrame(cursor.fetchall(), columns=['ID', 'BIRTHDATE', 'DEATHDATE', 'RACE', 'ETHNICITY', 'GENDER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to date\n",
    "df_patients[\"DEATHDATE\"] = pd.to_datetime(df_patients[\"DEATHDATE\"])\n",
    "df_patients[\"BIRTHDATE\"] = pd.to_datetime(df_patients[\"BIRTHDATE\"])\n",
    "# fill null values withh todays date\n",
    "df_patients['DEATHDATE'] = df_patients.DEATHDATE.fillna(pd.to_datetime(\"today\"))\n",
    "# calculate age\n",
    "df_patients[\"AGE\"] = df_patients.DEATHDATE.dt.year - df_patients.BIRTHDATE.dt.year\n",
    "# drop unnecessary variables\n",
    "df_patients = df_patients.drop(['BIRTHDATE', 'DEATHDATE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1326 entries, 0 to 1325\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ID         1326 non-null   object\n",
      " 1   RACE       1326 non-null   object\n",
      " 2   ETHNICITY  1326 non-null   object\n",
      " 3   GENDER     1326 non-null   object\n",
      " 4   AGE        1326 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 51.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_patients.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Duplicated Rows 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Duplicated Rows\", df_patients.duplicated(df_patients.columns).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid       name    type  notnull dflt_value  pk\n",
      "0    0         ID  STRING        0       None   1\n",
      "1    1       RACE  STRING        0       None   0\n",
      "2    2  ETHNICITY  STRING        0       None   0\n",
      "3    3     GENDER  STRING        0       None   0\n",
      "4    4        AGE   INT64        0       None   0\n"
     ]
    }
   ],
   "source": [
    "df_patients.to_sql('df_patients', connection, if_exists='replace', index=False)\n",
    "cursor.execute('INSERT INTO patients_ (ID, RACE, ETHNICITY, GENDER, AGE) SELECT ID, RACE, ETHNICITY, GENDER, RACE FROM df_patients;')\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_patients;''')\n",
    "\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(patients_)\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle observations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid         name    type  notnull dflt_value  pk\n",
      "0    0         CODE  STRING        0       None   1\n",
      "1    1  DESCRIPTION  STRING        0       None   0\n",
      "2    2        UNITS  STRING        0       None   0\n",
      "3    3         TYPE  STRING        0       None   0\n"
     ]
    }
   ],
   "source": [
    "# table observations_\n",
    "cursor.execute('''DROP TABLE IF EXISTS observations_;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE observations_ ( \n",
    "        CODE STRING PRIMARY KEY UNIQUE,\n",
    "        DESCRIPTION STRING,\n",
    "        UNITS STRING, \n",
    "        TYPE STRING\n",
    "        );''')\n",
    "\n",
    "# create df observation\n",
    "cursor.execute('''SELECT CODE, DESCRIPTION, UNITS, TYPE FROM OBSERVATIONS;''')\n",
    "df_observations = pd.DataFrame(cursor.fetchall(), columns=['CODE','DESCRIPTION', 'UNITS', 'TYPE'])\n",
    "\n",
    "df_observations = df_observations.drop_duplicates(subset='CODE')\n",
    "\n",
    "# transform dt in table\n",
    "df_observations.to_sql('df_observations', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('''INSERT INTO observations_ (CODE, DESCRIPTION, UNITS, TYPE) SELECT CODE, DESCRIPTION, UNITS, TYPE FROM df_observations;''')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_observations;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(observations_)\", connection))\n",
    "#print(pd.read_sql_query(\"SELECT * FROM observations_\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle careplans_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid         name    type  notnull dflt_value  pk\n",
      "0    0         CODE  STRING        0       None   1\n",
      "1    1  DESCRIPTION  STRING        0       None   0\n"
     ]
    }
   ],
   "source": [
    "# table careplans_code\n",
    "cursor.execute('''DROP TABLE IF EXISTS careplans_code;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE careplans_code ( \n",
    "        CODE STRING PRIMARY KEY UNIQUE,\n",
    "        DESCRIPTION STRING\n",
    "        );''')\n",
    "\n",
    "# create df careplans_code\n",
    "cursor.execute('''SELECT CODE, DESCRIPTION FROM CAREPLANS;''')\n",
    "df_careplans_code = pd.DataFrame(cursor.fetchall(), columns=['CODE','DESCRIPTION'])\n",
    "\n",
    "df_careplans_code = df_careplans_code.drop_duplicates(subset='CODE')\n",
    "\n",
    "# transform dt in table\n",
    "df_careplans_code.to_sql('df_careplans_code', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('''INSERT INTO careplans_code (CODE, DESCRIPTION) SELECT CODE, DESCRIPTION FROM df_careplans_code;''')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_careplans_code;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(careplans_code)\", connection))\n",
    "#print(pd.read_sql_query(\"SELECT * FROM careplans_code\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle careplans_reasoncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid               name    type  notnull dflt_value  pk\n",
      "0    0         REASONCODE  STRING        0       None   1\n",
      "1    1  REASONDESCRIPTION  STRING        0       None   0\n"
     ]
    }
   ],
   "source": [
    "# table careplans_reasoncode\n",
    "cursor.execute('''DROP TABLE IF EXISTS careplans_reasoncode;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE careplans_reasoncode ( \n",
    "        REASONCODE STRING PRIMARY KEY UNIQUE,\n",
    "        REASONDESCRIPTION STRING\n",
    "        );''')\n",
    "\n",
    "# create df careplans_code\n",
    "cursor.execute('''SELECT REASONCODE, REASONDESCRIPTION FROM CAREPLANS;''')\n",
    "df_careplans_reasoncode = pd.DataFrame(cursor.fetchall(), columns=['REASONCODE','REASONDESCRIPTION'])\n",
    "\n",
    "df_careplans_reasoncode = df_careplans_reasoncode.drop_duplicates(subset='REASONCODE')\n",
    "\n",
    "# transform dt in table\n",
    "df_careplans_reasoncode.to_sql('df_careplans_reasoncode', connection, if_exists='replace', index=False)\n",
    "   \n",
    "cursor.execute('INSERT INTO careplans_reasoncode (REASONCODE, REASONDESCRIPTION) SELECT REASONCODE, REASONDESCRIPTION FROM df_careplans_reasoncode;')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_careplans_reasoncode;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(careplans_reasoncode)\", connection))\n",
    "#print(pd.read_sql_query(\"SELECT * FROM careplans_reasoncode\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle conditions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid         name    type  notnull dflt_value  pk\n",
      "0    0         CODE  STRING        0       None   1\n",
      "1    1  DESCRIPTION  STRING        0       None   0\n"
     ]
    }
   ],
   "source": [
    "# table conditions\n",
    "cursor.execute('''DROP TABLE IF EXISTS conditions_;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE conditions_ ( \n",
    "        CODE STRING PRIMARY KEY UNIQUE,\n",
    "        DESCRIPTION STRING\n",
    "        );''')\n",
    "\n",
    "# create df conditions\n",
    "cursor.execute('''SELECT CODE, DESCRIPTION FROM CONDITIONS;''')\n",
    "df_conditions = pd.DataFrame(cursor.fetchall(), columns=['CODE','DESCRIPTION'])\n",
    "\n",
    "df_conditions = df_conditions.drop_duplicates(subset='CODE')\n",
    "\n",
    "# transform dt in table\n",
    "df_conditions.to_sql('df_conditions', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('INSERT INTO conditions_ (CODE, DESCRIPTION) SELECT CODE, DESCRIPTION FROM df_conditions;')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_conditions;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(conditions_)\", connection))\n",
    "#print(pd.read_sql_query(\"SELECT * FROM conditions_\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datumstabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def create_date_table(start='1900-01-01', end=datetime.today().strftime('%Y-%m-%d')):\n",
    "    \n",
    "    df_date = pd.DataFrame({\"Date\": pd.date_range(start, end)})\n",
    "\n",
    "    days_names = {\n",
    "        i: name\n",
    "        for i, name\n",
    "        in enumerate(['Monday', 'Tuesday', 'Wednesday',\n",
    "                      'Thursday', 'Friday', 'Saturday', \n",
    "                      'Sunday'])\n",
    "    }\n",
    "   \n",
    "    df_date[\"Day\"] = df_date.Date.dt.dayofweek.map(days_names.get)\n",
    "    df_date[\"Week\"] = df_date.Date.dt.weekofyear\n",
    "    df_date[\"Quarter\"] = df_date.Date.dt.quarter\n",
    "    df_date[\"Year\"] = df_date.Date.dt.year\n",
    "    df_date[\"Year_half\"] = (df_date.Quarter + 1) // 2\n",
    "    \n",
    "    return df_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f36dd0c3650>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table date_table\n",
    "cursor.execute('''DROP TABLE IF EXISTS date_table;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE date_table ( \n",
    "        DATE DATE PRIMARY KEY UNIQUE,\n",
    "        DAY STRING,\n",
    "        WEEK INT16,\n",
    "        QUARTER INT16,\n",
    "        YEAR INT16,\n",
    "        YEAR_HALF INT16\n",
    "        );''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid       name    type  notnull dflt_value  pk\n",
      "0    0       DATE    DATE        0       None   1\n",
      "1    1        DAY  STRING        0       None   0\n",
      "2    2       WEEK   INT16        0       None   0\n",
      "3    3    QUARTER   INT16        0       None   0\n",
      "4    4       YEAR   INT16        0       None   0\n",
      "5    5  YEAR_HALF   INT16        0       None   0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113338/3083742411.py:15: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  df_date[\"Week\"] = df_date.Date.dt.weekofyear\n"
     ]
    }
   ],
   "source": [
    "df_date = create_date_table()\n",
    "\n",
    "# transform dt in table\n",
    "df_date.to_sql('df_date', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('INSERT INTO date_table (DATE, DAY, WEEK, QUARTER, YEAR, YEAR_HALF) SELECT DATE, DAY, WEEK, QUARTER, YEAR, YEAR_HALF FROM df_date;')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_date;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(date_table)\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstelle Faktentabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('''DROP TABLE IF EXISTS fact_table;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE fact_table ( \n",
    "        PATIENT_ID STRING,\n",
    "        OBSERVATION_CODE STRING,\n",
    "        VALUE STRING,\n",
    "        DATE DATE,\n",
    "        CONDITIONS_CODE STRING,\n",
    "        ENDDATE DATE,\n",
    "        CAREPLANS_CODE STRING,\n",
    "        CAREPLANS_REASONCODE STRING,\n",
    "        FOREIGN KEY (PATIENT_ID)\n",
    "            REFERENCES patients_(ID),\n",
    "        FOREIGN KEY (OBSERVATION_CODE)\n",
    "            REFERENCES observations_(CODE),\n",
    "        FOREIGN KEY (CONDITIONS_CODE)\n",
    "            REFERENCES conditions_(CODE),\n",
    "        FOREIGN KEY (CAREPLANS_CODE)\n",
    "            REFERENCES careplans_code(CODE),\n",
    "        FOREIGN KEY (CAREPLANS_REASONCODE)\n",
    "            REFERENCES careplans_reasoncode(REASONCODE)\n",
    "        );''')\n",
    "\n",
    "cursor.execute('''INSERT INTO fact_table        \n",
    "                    (PATIENT_ID, OBSERVATION_CODE, VALUE, DATE) \n",
    "                    SELECT PATIENT, CODE, VALUE, DATE \n",
    "                    FROM OBSERVATIONS\n",
    "                    ;''')\n",
    "\n",
    "cursor.execute('''INSERT INTO fact_table        \n",
    "                    (PATIENT_ID, CAREPLANS_CODE, DATE, ENDDATE) \n",
    "                    SELECT PATIENT, CODE, START, STOP \n",
    "                    FROM CAREPLANS\n",
    "                    ;''')\n",
    "\n",
    "cursor.execute('''INSERT INTO fact_table        \n",
    "                    (PATIENT_ID, CAREPLANS_REASONCODE, DATE, ENDDATE) \n",
    "                    SELECT PATIENT, REASONCODE, START, STOP \n",
    "                    FROM CAREPLANS\n",
    "                    ;''')\n",
    "\n",
    "cursor.execute('''INSERT INTO fact_table        \n",
    "                    (PATIENT_ID, CONDITIONS_CODE, DATE, ENDDATE) \n",
    "                    SELECT PATIENT, CODE, START, STOP \n",
    "                    FROM CONDITIONS\n",
    "                    ;''')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  seq                 table                  from          to  on_update  \\\n",
      "0   0    0  careplans_reasoncode  CAREPLANS_REASONCODE  REASONCODE  NO ACTION   \n",
      "1   1    0        careplans_code        CAREPLANS_CODE        CODE  NO ACTION   \n",
      "2   2    0           conditions_       CONDITIONS_CODE        CODE  NO ACTION   \n",
      "3   3    0         observations_      OBSERVATION_CODE        CODE  NO ACTION   \n",
      "4   4    0             patients_            PATIENT_ID          ID  NO ACTION   \n",
      "\n",
      "   on_delete match  \n",
      "0  NO ACTION  NONE  \n",
      "1  NO ACTION  NONE  \n",
      "2  NO ACTION  NONE  \n",
      "3  NO ACTION  NONE  \n",
      "4  NO ACTION  NONE  \n"
     ]
    }
   ],
   "source": [
    "print(pd.read_sql_query(\"PRAGMA foreign_key_list(fact_table)\", connection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('patients',), ('careplans',), ('conditions',), ('observations',), ('anonnymized_patients',), ('anonnymized_careplans',), ('anonnymized_conditions',), ('anonnymized_observations',), ('patients_',), ('observations_',), ('careplans_code',), ('careplans_reasoncode',), ('conditions_',), ('date_table',), ('fact_table',)]\n"
     ]
    }
   ],
   "source": [
    "cursor.execute('SELECT name FROM sqlite_master where type=\"table\"')\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufräumen & Logs speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1657818136086, \"message\": \"⏳ Connected to db and created tables in 1853261ms\", \"params\": {\"timingInMilliseconds\": 1853261}}\n",
      "{\"type\": \"info\", \"time\": 1657818136087, \"message\": \"⏳ Loading data into db in 1856849ms\", \"params\": {\"timingInMilliseconds\": 1856849}}\n",
      "{\"type\": \"info\", \"time\": 1657818136087, \"message\": \"⏳ Writing annonymized patients in 1852616ms\", \"params\": {\"timingInMilliseconds\": 1852616}}\n",
      "{\"type\": \"info\", \"time\": 1657818136087, \"message\": \"⏳ Writing annonymized careplans in 1852679ms\", \"params\": {\"timingInMilliseconds\": 1852679}}\n",
      "{\"type\": \"info\", \"time\": 1657818136087, \"message\": \"⏳ Writing annonymized conditions in 1853792ms\", \"params\": {\"timingInMilliseconds\": 1853792}}\n",
      "{\"type\": \"info\", \"time\": 1657818136087, \"message\": \"⏳ Writing annonymized Observations in 1904065ms\", \"params\": {\"timingInMilliseconds\": 1904065}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lukas/git/DMA2022DataProjectC/src/ETL.ipynb Cell 50'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lukas/git/DMA2022DataProjectC/src/ETL.ipynb#ch0000068?line=0'>1</a>\u001b[0m connection\u001b[39m.\u001b[39mclose()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lukas/git/DMA2022DataProjectC/src/ETL.ipynb#ch0000068?line=1'>2</a>\u001b[0m logger\u001b[39m.\u001b[39;49mlogTimings()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lukas/git/DMA2022DataProjectC/src/ETL.ipynb#ch0000068?line=2'>3</a>\u001b[0m logger\u001b[39m.\u001b[39mwriteToFile(\u001b[39m\"\u001b[39m\u001b[39m../artefacts-for-release/etl-log.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/DMA2022DataProjectC/src/logger.py:39\u001b[0m, in \u001b[0;36mLogger.logTimings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogTimings\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     38\u001b[0m     \u001b[39mfor\u001b[39;00m timing \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimings:\n\u001b[0;32m---> 39\u001b[0m         usedTime \u001b[39m=\u001b[39m timing[\u001b[39m'\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m-\u001b[39;49m timing[\u001b[39m'\u001b[39;49m\u001b[39mstart\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     40\u001b[0m         message \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m⏳ \u001b[39m\u001b[39m{\u001b[39;00mtiming[\u001b[39m'\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{\u001b[39;00musedTime\u001b[39m}\u001b[39;00m\u001b[39mms\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogWithTiming(message, usedTime)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "connection.close()\n",
    "logger.logTimings()\n",
    "logger.writeToFile(\"../artefacts-for-release/etl-log.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
