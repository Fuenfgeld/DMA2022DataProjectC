{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genutzte Umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from logger import Logger\n",
    "from test_executer import TestExecutor\n",
    "import extract\n",
    "import sys\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "logger = Logger()\n",
    "testExecutor = TestExecutor(logger)\n",
    "\n",
    "dependencies = [\n",
    "    ('system', sys.version_info),\n",
    "    ('pandas', pd.__version__),\n",
    "    ('numpy', np.__version__),\n",
    "]\n",
    "for dependency in dependencies:\n",
    "    logger.log(f\"{dependency[0]} is installed with version {dependency[1]}\")\n",
    "\n",
    "logger.startTimeMeasurement('etl-process', 'Full etl process')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup der Daten\n",
    "\n",
    "Zuerst laden wir die benötigten Daten herunter und initialisieren die genutzten Python Objekte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\n",
    "    \"conditions\",\n",
    "    \"observations\",\n",
    "    \"patients\",\n",
    "]\n",
    "\n",
    "files = [\n",
    "    \"data/others/\",\n",
    "    \"data/asthma/\",\n",
    "    \"data/gallstones/\",\n",
    "    \"data/hypertension/\",\n",
    "]\n",
    "\n",
    "md5Hashes = {\n",
    "    \"data/others/conditions.csv\": \"ce0034e9ed9185b7d4c408ee9916de18\",\n",
    "    \"data/others/observations.csv\": \"b9e3bf1b033dc4af7f7ade78a48a50a4\",\n",
    "    \"data/others/patients.csv\": \"530570c8e30b77a822b37e927d1486b2\",\n",
    "    \"data/asthma/conditions.csv\": \"e7965095ec41ef88498540341c79c49e\",\n",
    "    \"data/asthma/observations.csv\": \"1b8583de62d4d9e80c224005d74dd736\",\n",
    "    \"data/asthma/patients.csv\": \"b139ef00c850308c3d3f8e7fa0f97724\",\n",
    "    \"data/gallstones/conditions.csv\": \"8a19bf13191cf074c64534c2fa01f15c\",\n",
    "    \"data/gallstones/observations.csv\": \"9d3807dc05cd7b4ccc3f0ee7b4f7b55e\",\n",
    "    \"data/gallstones/patients.csv\": \"3766f46941ee2155e0d1ed6e749e8ba7\",\n",
    "    \"data/hypertension/conditions.csv\": \"8310cdc07924b48e07aa841f9075b488\",\n",
    "    \"data/hypertension/observations.csv\": \"f7564c732eebe9ace17a46e50b3cc857\",\n",
    "    \"data/hypertension/patients.csv\": \"2ebdf6b168e9c968ffa949463cd074e7\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/allergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "def ensure_file_has_been_downloaded(filename):\n",
    "    full_filename = \"../\" + filename\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/Fuenfgeld/DMA2022DataProjectC/main/\" + filename\n",
    "    if os.path.isfile(full_filename):\n",
    "        logger.log(\"File {} already exists, skipping download\".format(filename))\n",
    "    else:\n",
    "        logger.log(\"Downloading {}\".format(filename))\n",
    "        download_file(url, full_filename)\n",
    "\n",
    "def download_file(url, filename):\n",
    "    with open(filename, 'wb') as out_file:\n",
    "        with urlopen(url) as file:\n",
    "            out_file.write(file.read())\n",
    "\n",
    "if not os.path.isfile(\"extract.py\"):\n",
    "    download_file(\n",
    "        \"https://raw.githubusercontent.com/Fuenfgeld/DMA2022DataProjectC/main/src/extract.py\",\n",
    "        \"extract.py\"\n",
    "    )\n",
    "\n",
    "dataChanged = False\n",
    "for file in files:\n",
    "    for table in tables:\n",
    "        filename = file+table+\".csv\"\n",
    "        ensure_file_has_been_downloaded(filename)\n",
    "\n",
    "        with open(\"../\" + filename) as fileHandle:\n",
    "            fileContent = fileHandle.read()\n",
    "            fileHandle.close()\n",
    "\n",
    "        md5Hash = hashlib.md5(fileContent.encode()).hexdigest()\n",
    "        if md5Hashes[filename] != md5Hash:\n",
    "            dataChanged = True\n",
    "    \n",
    "if dataChanged:\n",
    "    logger.log(\"❌ Data set changed\")\n",
    "else:\n",
    "    logger.log(\"✅ Using original data set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mit Datenbank verbinden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databaseFile = \"data.sqlite\"\n",
    "\n",
    "logger.startTimeMeasurement('open-db', 'Connected to db and created tables')\n",
    "connection = extract.connect_to_db(logger, databaseFile)  # create table patients, observations, conditions, careplans\n",
    "logger.endTimeMeasurement('open-db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sqliteConnection(_logger):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tablesInDb = list(map(lambda tableResult: tableResult[0], cursor.fetchall()))\n",
    "    tablesInDb.sort()\n",
    "\n",
    "    for table in tables:\n",
    "        if not(table in tablesInDb):\n",
    "            raise Exception('Table not found:', table)\n",
    "\n",
    "testExecutor.execute('Test connection to database', test_sqliteConnection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten in Datenbank laden\n",
    "\n",
    "Lade der verwendete Daten in die Datenbank:\n",
    "\n",
    "-   careplans\n",
    "-   conditions\n",
    "-   observations\n",
    "-   patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.startTimeMeasurement('load-data', 'Loading data into db')\n",
    "for file in files:\n",
    "    for table in tables:\n",
    "        extract.insert_values_to_table(logger, connection.cursor(), table, \"../\"+ file + table + \".csv\")  # TODO: insert ALL values in the right tables \n",
    "        connection.commit()\n",
    "\n",
    "logger.endTimeMeasurement('load-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annonymisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "logger.startTimeMeasurement('anonymization', 'Anonymizing all data sets')\n",
    "\n",
    "patientDf = pd.read_sql_query('SELECT * FROM patients;', connection)\n",
    "conditionsDf = pd.read_sql_query('SELECT * FROM conditions;', connection)\n",
    "observationsDf = pd.read_sql_query('SELECT * FROM observations;', connection)\n",
    "\n",
    "patientIds = [*patientDf.Id, *conditionsDf.PATIENT, *observationsDf.PATIENT]\n",
    "\n",
    "def test_sanityCheckCombiningIds(_logger): \n",
    "    expectedLen = len(patientDf) + len(conditionsDf) + len(observationsDf)\n",
    "    actualLen = len(patientIds)\n",
    "    if actualLen != expectedLen:\n",
    "        raise Exception('Not all patient ids were concatenated')\n",
    "\n",
    "testExecutor.execute('Sanity check: extracting all ids worked', test_sanityCheckCombiningIds)\n",
    "\n",
    "# Converts list to a set with only unique values\n",
    "uniqueIds = set(patientIds)\n",
    "logger.log(f\"{len(uniqueIds)} unique patient ids found\")\n",
    "if len(uniqueIds) >= len(patientDf.Id):\n",
    "    logger.log(f\"⚠️ The dataset contains {len(uniqueIds)} unique patientIds but only {len(patientDf.Id)} patients.\")\n",
    "\n",
    "anonymizedIds = {}\n",
    "for id in uniqueIds:\n",
    "    # Use uppercase here so it is easy to see if anonymized ids are used.\n",
    "    anonymizedIds[id] = hashlib.sha256(f\"{id}={random.random()}\".encode()).hexdigest().upper()\n",
    "\n",
    "def test_sanityEnsureAllIdsAreAnonymized(_logger): \n",
    "    for id in patientIds:\n",
    "        if id in anonymizedIds:\n",
    "            raise Exception('A origin id still exists in anonymized id list')\n",
    "\n",
    "testExecutor.execute('Sanity check: no origin ids exist anymore', test_sanityCheckCombiningIds)\n",
    "\n",
    "logger.startTimeMeasurement('anonymizedPatients', 'Writing anonymized patients')\n",
    "patientDf = patientDf.replace({\"Id\": anonymizedIds}).drop(columns=[\n",
    "    'SSN', 'DRIVERS', 'PASSPORT', 'PREFIX', 'FIRST', 'LAST', 'MAIDEN', 'BIRTHPLACE',\n",
    "    'ADDRESS', 'ZIP', 'LAT', 'LON'\n",
    "])\n",
    "patientDf.to_sql(name=\"anonymized_patients\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('anonymizedPatients')\n",
    "\n",
    "logger.startTimeMeasurement('anonymizedConditions', 'Writing anonymized conditions')\n",
    "conditionsDf = conditionsDf.replace({\"PATIENT\": anonymizedIds})\n",
    "conditionsDf.to_sql(name=\"anonymized_conditions\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('anonymizedConditions')\n",
    "\n",
    "logger.startTimeMeasurement('anonymizedObservations', 'Writing anonymized Observations')\n",
    "observationsDf = observationsDf.replace({\"PATIENT\": anonymizedIds})\n",
    "observationsDf.to_sql(name=\"anonymized_observations\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('anonymizedObservations')\n",
    "\n",
    "connection.execute('DROP TABLE patients;')\n",
    "connection.execute('DROP TABLE observations;')\n",
    "connection.execute('DROP TABLE conditions;')\n",
    "\n",
    "connection.commit()\n",
    "logger.endTimeMeasurement('anonymization')\n",
    "\n",
    "tables = ['anonymized_patients', 'anonymized_conditions', 'anonymized_observations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messung der Datenfehler\n",
    "\n",
    "Für unsere Forschungsfrage sind nur alle Daten mit gemessenen BMI relevant. Wurde dieser nicht vermessen oder eingetragen können die Daten für die Forschungsfrage nicht verwendet werden und sind somit unbrauchbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL-Values\n",
    "\n",
    "Die Rohdaten werden zuvor auf die Anzahl an NULL-Values überprüft. Weisen mehr als **ein drittel der Daten**   Lücken in der Codierung auf, wird ein Fehler in der Verfassung angenommen und die Daten müssen manuell Überprüft werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.startTimeMeasurement('null-check', 'Checking for NULL values')\n",
    "\n",
    "null_counter = 0\n",
    "num_of_elements = 0\n",
    "for table in tables:\n",
    "    querie = f\"SELECT * from {table};\"\n",
    "    df = pd.read_sql_query(querie,connection)\n",
    "    result_string = str(df.isna().sum()).replace(\"\\n\",\" NULL-Values in Column \")\n",
    "    logger.log(f\"Found {result_string} null-values in {table}.\")\n",
    "    null_counter = df.isna().sum().sum() + null_counter\n",
    "    num_of_elements = num_of_elements + df.size\n",
    "perc_null_val = round(null_counter / num_of_elements,3)\n",
    "\n",
    "if perc_null_val > 0.33:\n",
    "    logger.log(f\"Found {perc_null_val} null-values.\",type='Warning')\n",
    "else:\n",
    "    logger.log(f\"Found {perc_null_val} null-values.\")\n",
    "\n",
    "logger.endTimeMeasurement('null-check')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prüfung auf Duplikate\n",
    "\n",
    "Duplikate verfälschen die  Ergebnisse des Anlyseteil durch Steigerung der Grundgesamheit mit gleichen Werten. Somit müssen die Daten auf Duplikate in den einzelnen Files überprüft werden, um gleiche Messungen zu finden und gegebenfalls im ETL-Process zu entfernen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.startTimeMeasurement('duplicate-check', 'Checking for duplicate values')\n",
    "\n",
    "num_of_duplicates = 0\n",
    "num_of_elements = 0\n",
    "for table in tables:\n",
    "    querie = f\"SELECT * from {table};\"\n",
    "    df = pd.read_sql_query(querie,connection)\n",
    "    duplicates = df.groupby(df.columns.tolist()).size().reset_index().\\\n",
    "    rename(columns={0:'records'})\n",
    "    curr_num_duplicate = (duplicates.records -1).sum() \n",
    "    num_of_duplicates = num_of_duplicates + curr_num_duplicate\n",
    "    logger.log(f\"Found {curr_num_duplicate} duplicate-values in {table}.\")\n",
    "    num_of_elements = num_of_elements + df.size\n",
    "perc_duplicates = round(num_of_duplicates / num_of_elements,3)\n",
    "logger.log(f\"Found {perc_duplicates} duplicate-values.\")\n",
    "\n",
    "logger.endTimeMeasurement('duplicate-check')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prozentuales Anzahl von Gewichts und BMI Werten für Patieten\n",
    "\n",
    "Für unsere Forschungsfrage sind BMI-Werte relevant und müssen für den Patienten mindestes einmal codiert worden sein. Um die Forschungsfrage mit den zur vorliegenden Daten zu beanworten, sollten auch hier mindestens **ein drittel der Daten** mit einen BMI Codiert worden sein.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patients_query = \"\"\"\n",
    "SELECT COUNT(id) FROM anonymized_patients;\"\"\"\n",
    "count_bmi_query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(distinct id)\n",
    "FROM anonymized_patients patients\n",
    "JOIN anonymized_observations observations\n",
    "    ON patients.id == observations.patient\n",
    "WHERE observations.Code = '59576-9'\n",
    "\"\"\"\n",
    "\n",
    "count_all_bmi_query = f\"\"\"\n",
    "SELECT COUNT(patient) FROM anonymized_observations WHERE code = '59576-9'\"\"\"\n",
    "\n",
    "patient_all_count = connection.execute(all_patients_query).fetchall()[0][0]\n",
    "patient_bmi_count = connection.execute(count_bmi_query).fetchall()[0][0]\n",
    "bmi_count = connection.execute(count_all_bmi_query).fetchall()[0][0]\n",
    "ratio = round(patient_bmi_count/patient_all_count, 3) \n",
    "\n",
    "logger.log(f\"Total num of patients {patient_all_count}.\")\n",
    "\n",
    "if ratio > 0.33:\n",
    "    logger.log(f\"Found {patient_bmi_count} patients ({round(ratio*100,3)}%) with {bmi_count} BMI-values.\")\n",
    "else:\n",
    "    logger.log(f\"Found {patient_bmi_count} patients ({round(ratio*100,3)}%) with {bmi_count} BMI-values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorfiltern der Datensätze\n",
    "\n",
    "Um uns die weitere Verarbeitung der Daten zu erleichtern, entfernen wir nun alle Tabellenspalten\n",
    "und Datensätze, die wir in der Analyse nicht benötigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.startTimeMeasurement('filter-data', 'Remove unnecessary data for etl process')\n",
    "\n",
    "connection.execute('DROP TABLE IF EXISTS  filtered_patients;')\n",
    "connection.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS filtered_patients(\n",
    "        patient_id STRING PRIMARY KEY UNIQUE,\n",
    "        birth_date STRING,\n",
    "        death_date STRING\n",
    "    );\n",
    "''');\n",
    "connection.execute('''\n",
    "    INSERT INTO filtered_patients (patient_id, birth_date, death_date)\n",
    "        SELECT Id, BIRTHDATE, DEATHDATE FROM anonymized_patients;\n",
    "''')\n",
    "\n",
    "connection.execute('DROP TABLE IF EXISTS  filtered_conditions;')\n",
    "connection.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS filtered_conditions(\n",
    "        patient_id STRING,\n",
    "        code STRING,\n",
    "        description STRING,\n",
    "        start_date STRING,\n",
    "        end_date STRING\n",
    "    );\n",
    "''')\n",
    "connection.execute('''\n",
    "    INSERT INTO filtered_conditions (patient_id, code, description, start_date, end_date)\n",
    "        SELECT PATIENT, CODE, DESCRIPTION, START, STOP FROM anonymized_conditions\n",
    "        WHERE DESCRIPTION NOT LIKE '%finding%';\n",
    "''')\n",
    "\n",
    "connection.execute('DROP TABLE IF EXISTS  filtered_observations;')\n",
    "connection.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS filtered_observations(\n",
    "        patient_id STRING,\n",
    "        date STRING,\n",
    "        code STRING,\n",
    "        description STRING,\n",
    "        value STRING,\n",
    "        units STRING,\n",
    "        type STRING\n",
    "    );\n",
    "''')\n",
    "connection.execute('''\n",
    "    INSERT INTO filtered_observations (patient_id, date, code, description, value, units, type)\n",
    "        SELECT PATIENT, DATE, CODE, DESCRIPTION, VALUE, UNITS, TYPE FROM anonymized_observations\n",
    "        WHERE CODE in ('8302-2', '29463-7', '39156-5', '8462-4', '8480-6', '8867-4');\n",
    "''')\n",
    "\n",
    "connection.commit()\n",
    "logger.endTimeMeasurement('filter-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen Dimensionstabellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle patientDimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table patients_\n",
    "cursor.execute('''DROP TABLE IF EXISTS patientDimension;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE patientDimension ( \n",
    "        ID STRING PRIMARY KEY UNIQUE,\n",
    "        AGE INT64\n",
    "        );''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.startTimeMeasurement('fill-patient-dimension', 'Fill patient dimension table')\n",
    "\n",
    "# create df patients\n",
    "cursor.execute('''SELECT id, birthdate, deathdate FROM anonymized_patients;''')\n",
    "df_patients = pd.DataFrame(cursor.fetchall(), columns=['id', 'birthdate', 'deathdate'])\n",
    "\n",
    "# convert to date\n",
    "df_patients[\"deathdate\"] = pd.to_datetime(df_patients[\"deathdate\"])\n",
    "df_patients[\"birthdate\"] = pd.to_datetime(df_patients[\"birthdate\"])\n",
    "# fill null values withh todays date\n",
    "df_patients['deathdate'] = df_patients.deathdate.fillna(pd.to_datetime(\"today\"))\n",
    "# calculate age\n",
    "df_patients[\"age\"] = df_patients.deathdate.dt.year - df_patients.birthdate.dt.year\n",
    "# drop unnecessary variables\n",
    "df_patients = df_patients.drop(['birthdate', 'deathdate'], axis=1)\n",
    "\n",
    "logger.log(f\"Number of Duplicated Rows: {df_patients.duplicated(df_patients.columns).sum()}\")\n",
    "\n",
    "df_patients.to_sql('df_patients', connection, if_exists='replace', index=False)\n",
    "cursor.execute('INSERT INTO patientDimension (id, age) SELECT id, age FROM df_patients;')\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_patients;''')\n",
    "\n",
    "logger.endTimeMeasurement('fill-patient-dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle diseaseDimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table conditions\n",
    "cursor.execute('''DROP TABLE IF EXISTS diseaseDimension;''')\n",
    "cursor.execute('''\n",
    "    CREATE TABLE diseaseDimension ( \n",
    "    code STRING,\n",
    "    description STRING\n",
    ");''')\n",
    "\n",
    "logger.startTimeMeasurement('fill-disease-dimension', 'Fill disease dimension table')\n",
    "cursor.execute('''\n",
    "INSERT INTO diseaseDimension\n",
    "    SELECT DISTINCT code, description FROM filtered_conditions;\n",
    "''')\n",
    "cursor.close()\n",
    "connection.commit()\n",
    "\n",
    "logger.endTimeMeasurement('fill-disease-dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datumstabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def create_date_table(start='1900-01-01', end=datetime.today().strftime('%Y-%m-%d')):\n",
    "    \n",
    "    df_date = pd.DataFrame({\"Date\": pd.date_range(start, end)})\n",
    "\n",
    "    days_names = {\n",
    "        i: name\n",
    "        for i, name\n",
    "        in enumerate(['Monday', 'Tuesday', 'Wednesday',\n",
    "                      'Thursday', 'Friday', 'Saturday', \n",
    "                      'Sunday'])\n",
    "    }\n",
    "   \n",
    "    df_date[\"Day\"] = df_date.Date.dt.dayofweek.map(days_names.get)\n",
    "    df_date[\"Week\"] = df_date.Date.dt.weekofyear\n",
    "    df_date[\"Quarter\"] = df_date.Date.dt.quarter\n",
    "    df_date[\"Year\"] = df_date.Date.dt.year\n",
    "    df_date[\"Year_half\"] = (df_date.Quarter + 1) // 2\n",
    "    \n",
    "    return df_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table date_table\n",
    "cursor = connection.cursor()\n",
    "cursor.execute('''DROP TABLE IF EXISTS date_table;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE date_table ( \n",
    "        DATE DATE PRIMARY KEY UNIQUE,\n",
    "        DAY STRING,\n",
    "        WEEK INT16,\n",
    "        QUARTER INT16,\n",
    "        YEAR INT16,\n",
    "        YEAR_HALF INT16\n",
    "        );''')\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "df_date = create_date_table()\n",
    "\n",
    "# transform dt in table\n",
    "df_date.to_sql('df_date', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('INSERT INTO date_table (DATE, DAY, WEEK, QUARTER, YEAR, YEAR_HALF) SELECT DATE, DAY, WEEK, QUARTER, YEAR, YEAR_HALF FROM df_date;')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_date;''')\n",
    "cursor.close()\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstelle Faktentabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createValueTableForCode(code, name):\n",
    "    connection.execute(f'DROP TABLE IF EXISTS patient_condition_{name};')\n",
    "    connection.execute(f'''\n",
    "        CREATE TABLE patient_condition_{name} (\n",
    "            condition_row_id INT64,\n",
    "            patient_id STRING,\n",
    "            disease_code STRING,\n",
    "            {name} STRING,\n",
    "            date_diff STRING\n",
    "        );\n",
    "    ''')\n",
    "    connection.execute(f'''\n",
    "        INSERT INTO patient_condition_{name} (\n",
    "            condition_row_id,\n",
    "            patient_id,\n",
    "            disease_code,\n",
    "            {name},\n",
    "            date_diff\n",
    "        )\n",
    "        SELECT\n",
    "            rowid,\n",
    "            patient_id,\n",
    "            code,\n",
    "            {name},\n",
    "            MIN({name}_date_diff)\n",
    "        FROM (\n",
    "            SELECT\n",
    "                condition.rowid as rowid,\n",
    "                condition.patient_id as patient_id,\n",
    "                condition.code as code,\n",
    "                {name}_observation.value as {name},\n",
    "                ABS(JULIANDAY(condition.start_date) - JULIANDAY({name}_observation.date)) as {name}_date_diff\n",
    "            FROM filtered_conditions condition\n",
    "            LEFT JOIN filtered_observations {name}_observation ON\n",
    "                condition.patient_id = {name}_observation.patient_id\n",
    "                AND\n",
    "                {name}_observation.code = '{code}'\n",
    "        )\n",
    "        GROUP BY rowid;\n",
    "    ''')\n",
    "\n",
    "    connection.commit()\n",
    "\n",
    "logger.startTimeMeasurement('transform-facts', 'Transform facts for fact table')\n",
    "createValueTableForCode('39156-5', 'bmi')\n",
    "createValueTableForCode('8302-2', 'height')\n",
    "createValueTableForCode('29463-7', 'weight')\n",
    "createValueTableForCode('8462-4', 'diastolic_blood_pressure')\n",
    "createValueTableForCode('8480-6', 'systolic_blood_pressure')\n",
    "createValueTableForCode('8867-4', 'heart_rate')\n",
    "logger.endTimeMeasurement('transform-facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.execute('''DROP TABLE IF EXISTS fact_table;''')\n",
    "connection.execute('''\n",
    "        CREATE TABLE fact_table ( \n",
    "            patient_id STRING,\n",
    "            disease_code STRING,\n",
    "            bmi STRING,\n",
    "            height STRING,\n",
    "            weight STRING,\n",
    "            heart_rate STRING,\n",
    "            diastolic_blood_pressure STRING,\n",
    "            systolic_blood_pressure STRING\n",
    "        );''')\n",
    "\n",
    "logger.startTimeMeasurement('fill-fact-table', 'Fill fact table')\n",
    "connection.execute('''\n",
    "    INSERT INTO fact_table (\n",
    "        patient_id,\n",
    "        disease_code,\n",
    "        bmi,\n",
    "        height,\n",
    "        weight,\n",
    "        heart_rate,\n",
    "        diastolic_blood_pressure,\n",
    "        systolic_blood_pressure\n",
    "    ) SELECT \n",
    "        bmi.patient_id,\n",
    "        bmi.disease_code,\n",
    "        bmi.bmi,\n",
    "        height.height,\n",
    "        weight.weight,\n",
    "        heart_rate.heart_rate,\n",
    "        diastolic_blood_pressure.diastolic_blood_pressure,\n",
    "        systolic_blood_pressure.systolic_blood_pressure\n",
    "    FROM patient_condition_bmi bmi\n",
    "    JOIN patient_condition_height height\n",
    "        ON height.condition_row_id = bmi.condition_row_id\n",
    "    JOIN patient_condition_weight weight\n",
    "        ON weight.condition_row_id = bmi.condition_row_id\n",
    "    JOIN patient_condition_heart_rate heart_rate\n",
    "        ON heart_rate.condition_row_id = bmi.condition_row_id\n",
    "    JOIN patient_condition_diastolic_blood_pressure diastolic_blood_pressure\n",
    "        ON diastolic_blood_pressure.condition_row_id = bmi.condition_row_id\n",
    "    JOIN patient_condition_systolic_blood_pressure systolic_blood_pressure\n",
    "        ON systolic_blood_pressure.condition_row_id = bmi.condition_row_id;\n",
    "''')\n",
    "\n",
    "connection.execute('''\n",
    "    DELETE FROM fact_table\n",
    "    WHERE\n",
    "        bmi IS NULL\n",
    "        AND height IS NULL\n",
    "        AND weight IS NULL;\n",
    "''');\n",
    "\n",
    "connection.execute('''\n",
    "    UPDATE fact_table\n",
    "        SET bmi = weight / (height * height)\n",
    "    WHERE\n",
    "        bmi IS NULL\n",
    "        AND weight IS NOT NULL\n",
    "        AND height IS NOT NULL;\n",
    "''');\n",
    "\n",
    "connection.commit()\n",
    "logger.endTimeMeasurement('fill-fact-table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufräumen & Logs speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.endTimeMeasurement('etl-process')\n",
    "connection.close()\n",
    "logger.logTimings()\n",
    "logger.writeToFile(\"../artefacts-for-release/etl-log.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
