{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "<font size=\"20\">**‚ö†**</font> <font size=\"4\"> Nachfolgende Chunk muss nur in **Google-Coolab** ausgef√ºhrt werden und klont das Projekt-Repository ins Arbeitsverzeichniss, um auf die zus√§tzlichen Skript und CSV-Dateien zugreifen zu k√∂nnen. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Fuenfgeld/DMA2022DataProjectC.git\n",
    "# %cd DMA2022DataProjectC/src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inhaltsverzeichniss\n",
    "\n",
    "Der folgende [ETL-Prozess](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/ETL-Prozess-Verzeichniss) wird im Projekt-Wiki n√§her beschrieben:\n",
    "\n",
    "* [Einladen der Rohdaten und √ºberpr√ºfung auf Ver√§nderungen](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Datenvorverarbeitung#datenpr%C3%BCfung)\n",
    "* Verbindung und Pr√ºfung der Datenbank\n",
    "* [Anonymisierung von patientenbezogenen Daten](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Datenschutzfolgeabsch%C3%A4tzung#personenbezogene-daten)\n",
    "* [Messung der Datenfehler](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Datenqualit%C3%A4t)\n",
    "* [Filterung auf die relevante Daten](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Mappingtabellen)\n",
    "* [Transformation ins Stern Schenma](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Datenschema)\n",
    "\n",
    "![image](../images/filtered_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't know how to reset  #, please run `%reset?` for details\n",
      "Don't know how to reset  reset, please run `%reset?` for details\n",
      "Don't know how to reset  kernel, please run `%reset?` for details\n"
     ]
    }
   ],
   "source": [
    "%reset -f  # reset kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genutzte Umgebung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier werden die verwendeten Versionen von 'system', 'pandas' und 'numpy' in einem logger Objekt gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1658325470771, \"message\": \"system is installed with version sys.version_info(major=3, minor=8, micro=10, releaselevel='final', serial=0)\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325470771, \"message\": \"pandas is installed with version 1.4.3\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325470771, \"message\": \"numpy is installed with version 1.23.0\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from logger import Logger\n",
    "from test_executer import TestExecutor\n",
    "import extract\n",
    "import sys\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "logger = Logger()\n",
    "testExecutor = TestExecutor(logger)\n",
    "\n",
    "dependencies = [\n",
    "    ('system', sys.version_info),\n",
    "    ('pandas', pd.__version__),\n",
    "    ('numpy', np.__version__),\n",
    "]\n",
    "for dependency in dependencies:\n",
    "    logger.log(f\"{dependency[0]} is installed with version {dependency[1]}\") \n",
    "\n",
    "logger.startTimeMeasurement('etl-process', 'Full etl process')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup der Daten\n",
    "\n",
    "Zuerst laden wir die ben√∂tigten Daten herunter und initialisieren die genutzten Python Objekte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\n",
    "    \"conditions\",\n",
    "    \"observations\",\n",
    "    \"patients\",\n",
    "]\n",
    "\n",
    "files = [\n",
    "    \"data/others/\",\n",
    "    \"data/asthma/\",\n",
    "    \"data/gallstones/\",\n",
    "    \"data/hypertension/\",\n",
    "]\n",
    "\n",
    "md5Hashes = {\n",
    "    \"data/others/conditions.csv\": \"ce0034e9ed9185b7d4c408ee9916de18\",\n",
    "    \"data/others/observations.csv\": \"b9e3bf1b033dc4af7f7ade78a48a50a4\",\n",
    "    \"data/others/patients.csv\": \"530570c8e30b77a822b37e927d1486b2\",\n",
    "    \"data/asthma/conditions.csv\": \"e7965095ec41ef88498540341c79c49e\",\n",
    "    \"data/asthma/observations.csv\": \"1b8583de62d4d9e80c224005d74dd736\",\n",
    "    \"data/asthma/patients.csv\": \"b139ef00c850308c3d3f8e7fa0f97724\",\n",
    "    \"data/gallstones/conditions.csv\": \"8a19bf13191cf074c64534c2fa01f15c\",\n",
    "    \"data/gallstones/observations.csv\": \"9d3807dc05cd7b4ccc3f0ee7b4f7b55e\",\n",
    "    \"data/gallstones/patients.csv\": \"3766f46941ee2155e0d1ed6e749e8ba7\",\n",
    "    \"data/hypertension/conditions.csv\": \"8310cdc07924b48e07aa841f9075b488\",\n",
    "    \"data/hypertension/observations.csv\": \"f7564c732eebe9ace17a46e50b3cc857\",\n",
    "    \"data/hypertension/patients.csv\": \"2ebdf6b168e9c968ffa949463cd074e7\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird gepr√ºft, ob die Quelldaten bereits heruntergeladen wurden. Falls nicht, wird der download gestartet.<br>\n",
    "Au√üerdem wird gepr√ºft, ob sich die [Daten gegen√ºber den originalen Rohdaten ver√§ndert](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Datenvorverarbeitung#datenpr%C3%BCfung) haben.<br>\n",
    "Die Information werden im logger Objekt gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1658325470854, \"message\": \"File data/others/conditions.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325470855, \"message\": \"File data/others/observations.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325470881, \"message\": \"File data/others/patients.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325470882, \"message\": \"File data/asthma/conditions.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325470885, \"message\": \"File data/asthma/observations.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325470964, \"message\": \"File data/asthma/patients.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325470965, \"message\": \"File data/gallstones/conditions.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325470968, \"message\": \"File data/gallstones/observations.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325471039, \"message\": \"File data/gallstones/patients.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325471041, \"message\": \"File data/hypertension/conditions.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325471043, \"message\": \"File data/hypertension/observations.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325471127, \"message\": \"File data/hypertension/patients.csv already exists, skipping download\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325471128, \"message\": \"‚úÖ Using original data set\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "def ensure_file_has_been_downloaded(filename):\n",
    "    full_filename = \"../\" + filename\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/Fuenfgeld/DMA2022DataProjectC/main/\" + filename\n",
    "    if os.path.isfile(full_filename):\n",
    "        logger.log(\"File {} already exists, skipping download\".format(filename))\n",
    "    else:\n",
    "        logger.log(\"Downloading {}\".format(filename))\n",
    "        download_file(url, full_filename)\n",
    "\n",
    "def download_file(url, filename):\n",
    "    with open(filename, 'wb') as out_file:\n",
    "        with urlopen(url) as file:\n",
    "            out_file.write(file.read())\n",
    "\n",
    "if not os.path.isfile(\"extract.py\"):\n",
    "    download_file(\n",
    "        \"https://raw.githubusercontent.com/Fuenfgeld/DMA2022DataProjectC/main/src/extract.py\",\n",
    "        \"extract.py\"\n",
    "    )\n",
    "\n",
    "dataChanged = False\n",
    "for file in files:\n",
    "    for table in tables:\n",
    "        filename = file+table+\".csv\"\n",
    "        ensure_file_has_been_downloaded(filename)\n",
    "\n",
    "        with open(\"../\" + filename) as fileHandle:\n",
    "            fileContent = fileHandle.read()\n",
    "            fileHandle.close()\n",
    "\n",
    "        md5Hash = hashlib.md5(fileContent.encode()).hexdigest()\n",
    "        if md5Hashes[filename] != md5Hash:\n",
    "            dataChanged = True\n",
    "    \n",
    "if dataChanged:\n",
    "    logger.log(\"‚ùå Data set changed\")\n",
    "else:\n",
    "    logger.log(\"‚úÖ Using original data set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mit Datenbank verbinden\n",
    "\n",
    "Durch den Aufruf der Funktion 'connect_to_db', die in der Datei 'extract.py' definiert ist, werden die Quelltabellen in der Datenbank initialisiert .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "databaseFile = \"data.sqlite\"\n",
    "\n",
    "logger.startTimeMeasurement('open-db', 'Connected to db and created tables')\n",
    "connection = extract.connect_to_db(logger, databaseFile)  # create table patients, observations, conditions\n",
    "logger.endTimeMeasurement('open-db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test der Datenbankverbindung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1658325471179, \"message\": \"‚úÖ Test ran successfully: Test connection to database\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "def test_sqliteConnection(_logger):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tablesInDb = list(map(lambda tableResult: tableResult[0], cursor.fetchall()))\n",
    "    tablesInDb.sort()\n",
    "\n",
    "    for table in tables:\n",
    "        if not(table in tablesInDb):\n",
    "            raise Exception('Table not found:', table)\n",
    "\n",
    "testExecutor.execute('Test connection to database', test_sqliteConnection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten in Datenbank laden\n",
    "\n",
    "Laden der verwendete Daten in die Datenbank\n",
    "\n",
    "-   conditions\n",
    "-   observations\n",
    "-   patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1658325471204, \"message\": \"üèó Extracting data from ../data/others/conditions.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325471223, \"message\": \"üèó Extracting data from ../data/others/observations.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325471512, \"message\": \"üèó Extracting data from ../data/others/patients.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325471518, \"message\": \"üèó Extracting data from ../data/asthma/conditions.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325471620, \"message\": \"üèó Extracting data from ../data/asthma/observations.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325472654, \"message\": \"üèó Extracting data from ../data/asthma/patients.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325472661, \"message\": \"üèó Extracting data from ../data/gallstones/conditions.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325472696, \"message\": \"üèó Extracting data from ../data/gallstones/observations.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325473600, \"message\": \"üèó Extracting data from ../data/gallstones/patients.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325473609, \"message\": \"üèó Extracting data from ../data/hypertension/conditions.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325473711, \"message\": \"üèó Extracting data from ../data/hypertension/observations.csv\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325474776, \"message\": \"üèó Extracting data from ../data/hypertension/patients.csv\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "logger.startTimeMeasurement('load-data', 'Loading data into db')\n",
    "for file in files:\n",
    "    for table in tables:\n",
    "        extract.insert_values_to_table(logger, connection.cursor(), table, \"../\"+ file + table + \".csv\")  # TODO: insert ALL values in the right tables \n",
    "        connection.commit()\n",
    "\n",
    "logger.endTimeMeasurement('load-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonymisierung\n",
    "\n",
    "Zur Einhaltung der [Datenschutzfolgeabsch√§tzung](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Datenschutzfolgeabsch%C3%A4tzung) m√ºssen die patientenbezogenen Daten [anoymisiert](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Datenvorverarbeitung#anonymisierung) werden, sodass kein R√ºckschluss auf die Personen in darauf folgenden analysen gezogen werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1658325476118, \"message\": \"‚úÖ Test ran successfully: Sanity check: extracting all ids worked\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325476153, \"message\": \"1330 unique patient ids found\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325476153, \"message\": \"‚ö†Ô∏è The dataset contains 1330 unique patientIds but only 1326 patients.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325476155, \"message\": \"‚úÖ Test ran successfully: Sanity check: no origin ids exist anymore\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "logger.startTimeMeasurement('anonymization', 'Anonymizing all data sets')\n",
    "\n",
    "# create data frames\n",
    "patientDf = pd.read_sql_query('SELECT * FROM patients;', connection)\n",
    "conditionsDf = pd.read_sql_query('SELECT * FROM conditions;', connection)\n",
    "observationsDf = pd.read_sql_query('SELECT * FROM observations;', connection)\n",
    "\n",
    "# concatenate all existent patient ids\n",
    "patientIds = [*patientDf.Id, *conditionsDf.PATIENT, *observationsDf.PATIENT]\n",
    "\n",
    "# sanity check whether all patient ids were concatenated\n",
    "def test_sanityCheckCombiningIds(_logger): \n",
    "    expectedLen = len(patientDf) + len(conditionsDf) + len(observationsDf)\n",
    "    actualLen = len(patientIds)\n",
    "    if actualLen != expectedLen:\n",
    "        raise Exception('Not all patient ids were concatenated')\n",
    "\n",
    "testExecutor.execute('Sanity check: extracting all ids worked', test_sanityCheckCombiningIds)\n",
    "\n",
    "# converts list to a set with only unique values\n",
    "uniqueIds = set(patientIds)\n",
    "logger.log(f\"{len(uniqueIds)} unique patient ids found\")\n",
    "if len(uniqueIds) >= len(patientDf.Id):\n",
    "    logger.log(f\"‚ö†Ô∏è The dataset contains {len(uniqueIds)} unique patientIds but only {len(patientDf.Id)} patients.\")\n",
    "\n",
    "anonymizedIds = {}\n",
    "# anonymization\n",
    "for id in uniqueIds:\n",
    "    # use uppercase here so it is easy to see if anonymized ids are used\n",
    "    anonymizedIds[id] = hashlib.sha256(f\"{id}={random.random()}\".encode()).hexdigest().upper()\n",
    "\n",
    "# sanity check whether a origin id still exists in anonymized id list\n",
    "def test_sanityEnsureAllIdsAreAnonymized(_logger): \n",
    "    for id in patientIds:\n",
    "        if id in anonymizedIds:\n",
    "            raise Exception('A origin id still exists in anonymized id list')\n",
    "\n",
    "testExecutor.execute('Sanity check: no origin ids exist anymore', test_sanityCheckCombiningIds)\n",
    "\n",
    "# convert data frames in sql tables\n",
    "logger.startTimeMeasurement('anonymizedPatients', 'Writing anonymized patients')\n",
    "patientDf = patientDf.replace({\"Id\": anonymizedIds}).drop(columns=[\n",
    "    'SSN', 'DRIVERS', 'PASSPORT', 'PREFIX', 'FIRST', 'LAST', 'MAIDEN', 'BIRTHPLACE',\n",
    "    'ADDRESS', 'ZIP', 'LAT', 'LON'\n",
    "])\n",
    "patientDf.to_sql(name=\"anonymized_patients\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('anonymizedPatients')\n",
    "\n",
    "logger.startTimeMeasurement('anonymizedConditions', 'Writing anonymized conditions')\n",
    "conditionsDf = conditionsDf.replace({\"PATIENT\": anonymizedIds})\n",
    "conditionsDf.to_sql(name=\"anonymized_conditions\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('anonymizedConditions')\n",
    "\n",
    "logger.startTimeMeasurement('anonymizedObservations', 'Writing anonymized Observations')\n",
    "observationsDf = observationsDf.replace({\"PATIENT\": anonymizedIds})\n",
    "observationsDf.to_sql(name=\"anonymized_observations\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('anonymizedObservations')\n",
    "\n",
    "connection.execute('DROP TABLE patients;')\n",
    "connection.execute('DROP TABLE observations;')\n",
    "connection.execute('DROP TABLE conditions;')\n",
    "\n",
    "connection.commit()\n",
    "logger.endTimeMeasurement('anonymization')\n",
    "\n",
    "tables = ['anonymized_patients', 'anonymized_conditions', 'anonymized_observations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√ºr eine bessere √úbersicht lassen sich mit folgendem Befehl die in der Datenbank enthaltenen Tabellen anzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      name\n",
      "0    anonymized_conditions\n",
      "1  anonymized_observations\n",
      "2      anonymized_patients\n"
     ]
    }
   ],
   "source": [
    "print(pd.read_sql_query('''SELECT name FROM sqlite_master \n",
    "WHERE type IN ('table','view') \n",
    "AND name NOT LIKE 'sqlite_%'\n",
    "ORDER BY 1''', connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messung der Datenfehler\n",
    "\n",
    "Die [Datenqualit√§t](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Datenqualit%C3%A4t) ist von wichtiger Bedeutung und gibt an ob die Daten f√ºr die weitere Auswertung √ºberhaupt zu gebrauchen sind. Zum Beispiel sind f√ºr unsere [Forschungsfrage](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki) ausschlie√ülich Daten mit gemessenen BMI relevant. Wurde dieser nicht vermessen oder eingetragen, so k√∂nnen die Daten f√ºr die [Forschungsfrage](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki) nicht verwendet werden und sind somit unbrauchbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anzahl NULL-Values\n",
    "\n",
    "Die Rohdaten werden zuvor auf die [Anzahl an NULL-Values √ºberpr√ºft](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Datenqualit%C3%A4t#prozentualer-anteil-von-null-werten-im-jedem-merkmal).<br>\n",
    "Weisen mehr als **ein drittel der Daten** L√ºcken in der Kodierung auf, wird ein Fehler in der Verfassung angenommen und die Daten m√ºssen manuell √ºberpr√ºft werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1658325528782, \"message\": \"Found index                  0 NULL-Values in Column Id                     0 NULL-Values in Column BIRTHDATE              0 NULL-Values in Column DEATHDATE              0 NULL-Values in Column SUFFIX                 0 NULL-Values in Column MARITAL                0 NULL-Values in Column RACE                   0 NULL-Values in Column ETHNICITY              0 NULL-Values in Column GENDER                 0 NULL-Values in Column CITY                   0 NULL-Values in Column STATE                  0 NULL-Values in Column COUNTRY                0 NULL-Values in Column HEALTHCARE_EXPENSES    0 NULL-Values in Column HEALTHCARE_COVERAGE    0 NULL-Values in Column dtype: int64 null-values in anonymized_patients.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325528870, \"message\": \"Found index          0 NULL-Values in Column START          0 NULL-Values in Column STOP           0 NULL-Values in Column PATIENT        0 NULL-Values in Column ENCOUNTER      0 NULL-Values in Column CODE           0 NULL-Values in Column DESCRIPTION    0 NULL-Values in Column dtype: int64 null-values in anonymized_conditions.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325530866, \"message\": \"Found index               0 NULL-Values in Column DATE                0 NULL-Values in Column PATIENT             0 NULL-Values in Column ENCOUNTER           0 NULL-Values in Column OBSERVATION_TYPE    0 NULL-Values in Column CODE                0 NULL-Values in Column DESCRIPTION         0 NULL-Values in Column VALUE               0 NULL-Values in Column UNITS               0 NULL-Values in Column TYPE                0 NULL-Values in Column dtype: int64 null-values in anonymized_observations.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325531582, \"message\": \"Found 0.0 null-values.\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "logger.startTimeMeasurement('null-check', 'Checking for NULL values')\n",
    "\n",
    "null_counter = 0\n",
    "num_of_elements = 0\n",
    "for table in tables:\n",
    "    querie = f\"SELECT * from {table};\"\n",
    "    df = pd.read_sql_query(querie,connection)\n",
    "    result_string = str(df.isna().sum()).replace(\"\\n\",\" NULL-Values in Column \")\n",
    "    logger.log(f\"Found {result_string} null-values in {table}.\")\n",
    "    null_counter = df.isna().sum().sum() + null_counter\n",
    "    num_of_elements = num_of_elements + df.size\n",
    "perc_null_val = round(null_counter / num_of_elements,3)\n",
    "\n",
    "if perc_null_val > 0.33:\n",
    "    logger.log(f\"Found {perc_null_val} null-values.\",type='Warning')\n",
    "else:\n",
    "    logger.log(f\"Found {perc_null_val} null-values.\")\n",
    "\n",
    "logger.endTimeMeasurement('null-check')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr√ºfung auf Duplikate\n",
    "\n",
    "Duplikate verf√§lschen die Ergebnisse des Analyseteil durch Steigerung der Grundgesamheit mit gleichen Werten. Somit m√ºssen die Daten auf [Duplikate in den einzelnen Files √ºberpr√ºft](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Datenqualit%C3%A4t#pr%C3%BCfung-auf-duplikate) werden, um gleiche Messungen zu finden und gegebenfalls im ETL-Process zu entfernen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1658325531781, \"message\": \"Found 0 duplicate-values in anonymized_patients.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325531911, \"message\": \"Found 0 duplicate-values in anonymized_conditions.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325534404, \"message\": \"Found 0 duplicate-values in anonymized_observations.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325534404, \"message\": \"Found 0.0 duplicate-values.\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "logger.startTimeMeasurement('duplicate-check', 'Checking for duplicate values')\n",
    "\n",
    "num_of_duplicates = 0\n",
    "num_of_elements = 0\n",
    "for table in tables:\n",
    "    querie = f\"SELECT * from {table};\"\n",
    "    df = pd.read_sql_query(querie,connection)\n",
    "    duplicates = df.groupby(df.columns.tolist()).size().reset_index().\\\n",
    "    rename(columns={0:'records'})\n",
    "    curr_num_duplicate = (duplicates.records -1).sum() \n",
    "    num_of_duplicates = num_of_duplicates + curr_num_duplicate\n",
    "    logger.log(f\"Found {curr_num_duplicate} duplicate-values in {table}.\")\n",
    "    num_of_elements = num_of_elements + df.size\n",
    "perc_duplicates = round(num_of_duplicates / num_of_elements,3)\n",
    "logger.log(f\"Found {perc_duplicates} duplicate-values.\")\n",
    "\n",
    "logger.endTimeMeasurement('duplicate-check')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prozentualer Anteil von Gewichts und BMI Werten f√ºr Patienten\n",
    "\n",
    "F√ºr unsere [Forschungsfrage](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki) sind [BMI-Werte relevant](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Datenqualit%C3%A4t#prozentualer-anteil-an-gewichts--und-bmi-werten-im-datensatz) und m√ºssen f√ºr den Patienten mindestes einmal kodiert worden sein. Um die [Forschungsfrage](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki) mit den zur vorliegenden Daten zu beanworten, sollten auch hier mindestens **ein drittel der Daten** mit einen BMI kodiert worden sein.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1658325534540, \"message\": \"Total num of patients 1326.\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325534540, \"message\": \"Found 450 patients (33.9%) with 3539 BMI-values.\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "all_patients_query = \"\"\"\n",
    "SELECT COUNT(id) FROM anonymized_patients;\"\"\"\n",
    "count_bmi_query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(distinct id)\n",
    "FROM anonymized_patients patients\n",
    "JOIN anonymized_observations observations\n",
    "    ON patients.id == observations.patient\n",
    "WHERE observations.Code = '59576-9'\n",
    "\"\"\"\n",
    "\n",
    "count_all_bmi_query = f\"\"\"\n",
    "SELECT COUNT(patient) FROM anonymized_observations WHERE code = '59576-9'\"\"\"\n",
    "\n",
    "patient_all_count = connection.execute(all_patients_query).fetchall()[0][0]\n",
    "patient_bmi_count = connection.execute(count_bmi_query).fetchall()[0][0]\n",
    "bmi_count = connection.execute(count_all_bmi_query).fetchall()[0][0]\n",
    "ratio = round(patient_bmi_count/patient_all_count, 3) \n",
    "\n",
    "logger.log(f\"Total num of patients {patient_all_count}.\")\n",
    "\n",
    "if ratio > 0.33:\n",
    "    logger.log(f\"Found {patient_bmi_count} patients ({round(ratio*100,3)}%) with {bmi_count} BMI-values.\")\n",
    "else:\n",
    "    logger.log(f\"Found {patient_bmi_count} patients ({round(ratio*100,3)}%) with {bmi_count} BMI-values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Vorfilterung der Datens√§tze](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Datenvorverarbeitung#vergr%C3%B6berung-der-daten)\n",
    "\n",
    "Um uns die weitere Verarbeitung der Daten zu erleichtern, entfernen wir alle Tabellenspalten und Datens√§tze, die wir in der Analyse nicht ben√∂tigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.startTimeMeasurement('filter-data', 'Remove unnecessary data for etl process')\n",
    "\n",
    "connection.execute('DROP TABLE IF EXISTS  filtered_patients;')\n",
    "connection.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS filtered_patients(\n",
    "        patient_id STRING PRIMARY KEY UNIQUE,\n",
    "        birth_date STRING,\n",
    "        death_date STRING\n",
    "    );\n",
    "''');\n",
    "connection.execute('''\n",
    "    INSERT INTO filtered_patients (patient_id, birth_date, death_date)\n",
    "        SELECT Id, BIRTHDATE, DEATHDATE FROM anonymized_patients;\n",
    "''')\n",
    "\n",
    "connection.execute('DROP TABLE IF EXISTS  filtered_conditions;')\n",
    "connection.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS filtered_conditions(\n",
    "        patient_id STRING,\n",
    "        code STRING,\n",
    "        description STRING,\n",
    "        start_date STRING,\n",
    "        end_date STRING\n",
    "    );\n",
    "''')\n",
    "connection.execute('''\n",
    "    INSERT INTO filtered_conditions (patient_id, code, description, start_date, end_date)\n",
    "        SELECT PATIENT, CODE, DESCRIPTION, START, STOP FROM anonymized_conditions\n",
    "        WHERE DESCRIPTION NOT LIKE '%finding%';\n",
    "''')\n",
    "\n",
    "connection.execute('DROP TABLE IF EXISTS  filtered_observations;')\n",
    "connection.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS filtered_observations(\n",
    "        patient_id STRING,\n",
    "        date STRING,\n",
    "        code STRING,\n",
    "        description STRING,\n",
    "        value STRING,\n",
    "        units STRING,\n",
    "        type STRING\n",
    "    );\n",
    "''')\n",
    "connection.execute('''\n",
    "    INSERT INTO filtered_observations (patient_id, date, code, description, value, units, type)\n",
    "        SELECT PATIENT, DATE, CODE, DESCRIPTION, VALUE, UNITS, TYPE FROM anonymized_observations\n",
    "        WHERE CODE in ('8302-2', '29463-7', '39156-5', '8462-4', '8480-6', '8867-4');\n",
    "''')\n",
    "\n",
    "connection.commit()\n",
    "logger.endTimeMeasurement('filter-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star Schema\n",
    "\n",
    "Im folgenden werden die vorverabeiten Tabellen nun in das Stern-Schema √ºberf√ºhrt. Wodruch k√∂nnen die Abfragen und [Analysen](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Analyse) der Daten effizent durchgef√ºhrt werden.\n",
    "\n",
    "![](../images/star_schema.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionstabelle _patientDimension_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7fd6ea97ec00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table patients_\n",
    "cursor.execute('''DROP TABLE IF EXISTS patientDimension;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE patientDimension ( \n",
    "        ID STRING PRIMARY KEY UNIQUE,\n",
    "        AGE INT64\n",
    "        );''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Abschnitt wird aus dem Geburts- und Todesdatum, bzw. aus dem Geburtsdatum und dem heutigen Tag das Alter jedes Patienten berechnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1658325534805, \"message\": \"Number of Duplicated Rows: 0\", \"params\": null}\n"
     ]
    }
   ],
   "source": [
    "logger.startTimeMeasurement('fill-patient-dimension', 'Fill patient dimension table')\n",
    "\n",
    "# create df patients\n",
    "cursor.execute('''SELECT id, birthdate, deathdate FROM anonymized_patients;''')\n",
    "df_patients = pd.DataFrame(cursor.fetchall(), columns=['id', 'birthdate', 'deathdate'])\n",
    "\n",
    "# convert to date\n",
    "df_patients[\"deathdate\"] = pd.to_datetime(df_patients[\"deathdate\"])\n",
    "df_patients[\"birthdate\"] = pd.to_datetime(df_patients[\"birthdate\"])\n",
    "# fill null values withh todays date\n",
    "df_patients['deathdate'] = df_patients.deathdate.fillna(pd.to_datetime(\"today\"))\n",
    "# calculate age\n",
    "df_patients[\"age\"] = df_patients.deathdate.dt.year - df_patients.birthdate.dt.year\n",
    "# drop unnecessary variables\n",
    "df_patients = df_patients.drop(['birthdate', 'deathdate'], axis=1)\n",
    "\n",
    "logger.log(f\"Number of Duplicated Rows: {df_patients.duplicated(df_patients.columns).sum()}\")\n",
    "\n",
    "df_patients.to_sql('df_patients', connection, if_exists='replace', index=False)\n",
    "cursor.execute('INSERT INTO patientDimension (id, age) SELECT id, age FROM df_patients;')\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_patients;''')\n",
    "\n",
    "logger.endTimeMeasurement('fill-patient-dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionstabelle _diseaseDimension_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table conditions\n",
    "cursor.execute('''DROP TABLE IF EXISTS diseaseDimension;''')\n",
    "cursor.execute('''\n",
    "    CREATE TABLE diseaseDimension ( \n",
    "    code STRING,\n",
    "    description STRING\n",
    ");''')\n",
    "\n",
    "logger.startTimeMeasurement('fill-disease-dimension', 'Fill disease dimension table')\n",
    "cursor.execute('''\n",
    "INSERT INTO diseaseDimension\n",
    "    SELECT DISTINCT code, description FROM filtered_conditions;\n",
    "''')\n",
    "cursor.close()\n",
    "connection.commit()\n",
    "\n",
    "logger.endTimeMeasurement('fill-disease-dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faktentabelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√ºr jedes Merkmal\n",
    "* BMI\n",
    "* Gr√∂√üe\n",
    "* Gewicht\n",
    "* diastolischer Blutdruck\n",
    "* systolischer Blutdruck\n",
    "* Herzfrequenz\n",
    "\n",
    "wird eine Zwischentabelle angelegt, die als Hilfestellungen zum F√ºllen der Faktentabelle dienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createValueTableForCode(code, name):\n",
    "    connection.execute(f'DROP TABLE IF EXISTS patient_condition_{name};')\n",
    "    connection.execute(f'''\n",
    "        CREATE TABLE patient_condition_{name} (\n",
    "            condition_row_id INT64,\n",
    "            patient_id STRING,\n",
    "            disease_code STRING,\n",
    "            {name} STRING,\n",
    "            date_diff STRING\n",
    "        );\n",
    "    ''')\n",
    "    connection.execute(f'''\n",
    "        INSERT INTO patient_condition_{name} (\n",
    "            condition_row_id,\n",
    "            patient_id,\n",
    "            disease_code,\n",
    "            {name},\n",
    "            date_diff\n",
    "        )\n",
    "        SELECT\n",
    "            rowid,\n",
    "            patient_id,\n",
    "            code,\n",
    "            {name},\n",
    "            MIN({name}_date_diff)\n",
    "        FROM (\n",
    "            SELECT\n",
    "                condition.rowid as rowid,\n",
    "                condition.patient_id as patient_id,\n",
    "                condition.code as code,\n",
    "                {name}_observation.value as {name},\n",
    "                ABS(JULIANDAY(condition.start_date) - JULIANDAY({name}_observation.date)) as {name}_date_diff\n",
    "            FROM filtered_conditions condition\n",
    "            LEFT JOIN filtered_observations {name}_observation ON\n",
    "                condition.patient_id = {name}_observation.patient_id\n",
    "                AND\n",
    "                {name}_observation.code = '{code}'\n",
    "        )\n",
    "        GROUP BY rowid;\n",
    "    ''')\n",
    "\n",
    "    connection.commit()\n",
    "\n",
    "logger.startTimeMeasurement('transform-facts', 'Transform facts for fact table')\n",
    "createValueTableForCode('39156-5', 'bmi')\n",
    "createValueTableForCode('8302-2', 'height')\n",
    "createValueTableForCode('29463-7', 'weight')\n",
    "createValueTableForCode('8462-4', 'diastolic_blood_pressure')\n",
    "createValueTableForCode('8480-6', 'systolic_blood_pressure')\n",
    "createValueTableForCode('8867-4', 'heart_rate')\n",
    "logger.endTimeMeasurement('transform-facts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisieren und F√ºllen der Faktentabelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.execute('''DROP TABLE IF EXISTS fact_table;''')\n",
    "connection.execute('''\n",
    "        CREATE TABLE fact_table ( \n",
    "            patient_id STRING,\n",
    "            disease_code STRING,\n",
    "            bmi STRING,\n",
    "            height STRING,\n",
    "            weight STRING,\n",
    "            heart_rate STRING,\n",
    "            diastolic_blood_pressure STRING,\n",
    "            systolic_blood_pressure STRING\n",
    "        );''')\n",
    "\n",
    "logger.startTimeMeasurement('fill-fact-table', 'Fill fact table')\n",
    "connection.execute('''\n",
    "    INSERT INTO fact_table (\n",
    "        patient_id,\n",
    "        disease_code,\n",
    "        bmi,\n",
    "        height,\n",
    "        weight,\n",
    "        heart_rate,\n",
    "        diastolic_blood_pressure,\n",
    "        systolic_blood_pressure\n",
    "    ) SELECT \n",
    "        bmi.patient_id,\n",
    "        bmi.disease_code,\n",
    "        bmi.bmi,\n",
    "        height.height,\n",
    "        weight.weight,\n",
    "        heart_rate.heart_rate,\n",
    "        diastolic_blood_pressure.diastolic_blood_pressure,\n",
    "        systolic_blood_pressure.systolic_blood_pressure\n",
    "    FROM patient_condition_bmi bmi\n",
    "    JOIN patient_condition_height height\n",
    "        ON height.condition_row_id = bmi.condition_row_id\n",
    "    JOIN patient_condition_weight weight\n",
    "        ON weight.condition_row_id = bmi.condition_row_id\n",
    "    JOIN patient_condition_heart_rate heart_rate\n",
    "        ON heart_rate.condition_row_id = bmi.condition_row_id\n",
    "    JOIN patient_condition_diastolic_blood_pressure diastolic_blood_pressure\n",
    "        ON diastolic_blood_pressure.condition_row_id = bmi.condition_row_id\n",
    "    JOIN patient_condition_systolic_blood_pressure systolic_blood_pressure\n",
    "        ON systolic_blood_pressure.condition_row_id = bmi.condition_row_id;\n",
    "''')\n",
    "\n",
    "connection.execute('''\n",
    "    DELETE FROM fact_table\n",
    "    WHERE\n",
    "        bmi IS NULL\n",
    "        AND height IS NULL\n",
    "        AND weight IS NULL;\n",
    "''')\n",
    "\n",
    "connection.execute('''\n",
    "    UPDATE fact_table\n",
    "        SET bmi = weight / (height * height)\n",
    "    WHERE\n",
    "        bmi IS NULL\n",
    "        AND weight IS NOT NULL\n",
    "        AND height IS NOT NULL;\n",
    "''')\n",
    "\n",
    "connection.commit()\n",
    "logger.endTimeMeasurement('fill-fact-table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufr√§umen & Logs speichern\n",
    "\n",
    "Zum Schluss wird die Logg-Datei f√ºr die Nachverfolgbarkeit archiviert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Full etl process in 507122ms\", \"params\": {\"timingInMilliseconds\": 507122}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Connected to db and created tables in 442026ms\", \"params\": {\"timingInMilliseconds\": 442026}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Loading data into db in 445599ms\", \"params\": {\"timingInMilliseconds\": 445599}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Anonymizing all data sets in 496011ms\", \"params\": {\"timingInMilliseconds\": 496011}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Writing anonymized patients in 442355ms\", \"params\": {\"timingInMilliseconds\": 442355}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Writing anonymized conditions in 443570ms\", \"params\": {\"timingInMilliseconds\": 443570}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Writing anonymized Observations in 492235ms\", \"params\": {\"timingInMilliseconds\": 492235}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"Time measurement for etl-process not finished\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"Time measurement for open-db not finished\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"Time measurement for load-data not finished\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"Time measurement for anonymization not finished\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"Time measurement for anonymizedPatients not finished\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"Time measurement for anonymizedConditions not finished\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"Time measurement for anonymizedObservations not finished\", \"params\": null}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Checking for NULL values in 2806ms\", \"params\": {\"timingInMilliseconds\": 2806}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Checking for duplicate values in 2793ms\", \"params\": {\"timingInMilliseconds\": 2793}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Remove unnecessary data for etl process in 166ms\", \"params\": {\"timingInMilliseconds\": 166}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Fill patient dimension table in 14ms\", \"params\": {\"timingInMilliseconds\": 14}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Fill disease dimension table in 5ms\", \"params\": {\"timingInMilliseconds\": 5}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Transform facts for fact table in 977ms\", \"params\": {\"timingInMilliseconds\": 977}}\n",
      "{\"type\": \"info\", \"time\": 1658325535915, \"message\": \"‚è≥ Fill fact table in 34ms\", \"params\": {\"timingInMilliseconds\": 34}}\n"
     ]
    }
   ],
   "source": [
    "logger.endTimeMeasurement('etl-process')\n",
    "connection.close()\n",
    "logger.logTimings()\n",
    "logger.writeToFile(\"../artefacts-for-release/etl-log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse\n",
    "\n",
    "Anschlie√üend kann die Analyse auf den Daten durchgef√ºhrt werden. Das Notebook hierzu finden Sie [hier](https://github.com/Fuenfgeld/DMA2022DataProjectC/blob/main/src/analysis.ipynb) oder eine schrieftliche Ausarbeitung [hier](https://github.com/Fuenfgeld/DMA2022DataProjectC/wiki/Analyse)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
