{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genutzte Umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from logger import Logger\n",
    "from test_executer import TestExecutor\n",
    "import extract\n",
    "import sys\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "logger = Logger()\n",
    "testExecutor = TestExecutor(logger)\n",
    "\n",
    "dependencies = [\n",
    "    ('system', sys.version_info),\n",
    "    ('pandas', pd.__version__),\n",
    "    ('numpy', np.__version__),\n",
    "]\n",
    "for dependency in dependencies:\n",
    "    logger.log(f\"{dependency[0]} is installed with version {dependency[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup der Daten\n",
    "\n",
    "Zuerst laden wir die benötigten Daten herunter und initialisieren die genutzten Python Objekte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\n",
    "    \"careplans\",\n",
    "    \"conditions\",\n",
    "    \"observations\",\n",
    "    \"patients\",\n",
    "]\n",
    "\n",
    "files = [\n",
    "    \"data/others/\",\n",
    "    \"data/asthma/\",\n",
    "    \"data/gallstones/\",\n",
    "    \"data/hypertension/\",\n",
    "]\n",
    "\n",
    "md5Hashes = {\n",
    "    \"data/others/careplans.csv\": \"365403e27541792755361bc0f6125506\",\n",
    "    \"data/others/conditions.csv\": \"ce0034e9ed9185b7d4c408ee9916de18\",\n",
    "    \"data/others/observations.csv\": \"b9e3bf1b033dc4af7f7ade78a48a50a4\",\n",
    "    \"data/others/patients.csv\": \"530570c8e30b77a822b37e927d1486b2\",\n",
    "    \"data/asthma/careplans.csv\": \"3c4aff1d0d576de6624c3726ec2dd544\",\n",
    "    \"data/asthma/conditions.csv\": \"e7965095ec41ef88498540341c79c49e\",\n",
    "    \"data/asthma/observations.csv\": \"1b8583de62d4d9e80c224005d74dd736\",\n",
    "    \"data/asthma/patients.csv\": \"b139ef00c850308c3d3f8e7fa0f97724\",\n",
    "    \"data/gallstones/careplans.csv\": \"35399fb01b2771b770c2f9f312e62dc2\",\n",
    "    \"data/gallstones/conditions.csv\": \"8a19bf13191cf074c64534c2fa01f15c\",\n",
    "    \"data/gallstones/observations.csv\": \"9d3807dc05cd7b4ccc3f0ee7b4f7b55e\",\n",
    "    \"data/gallstones/patients.csv\": \"3766f46941ee2155e0d1ed6e749e8ba7\",\n",
    "    \"data/hypertension/careplans.csv\": \"ddf053c4e56f24c4ce28e6d57edfd8b1\",\n",
    "    \"data/hypertension/conditions.csv\": \"8310cdc07924b48e07aa841f9075b488\",\n",
    "    \"data/hypertension/observations.csv\": \"f7564c732eebe9ace17a46e50b3cc857\",\n",
    "    \"data/hypertension/patients.csv\": \"2ebdf6b168e9c968ffa949463cd074e7\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/allergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "def ensure_file_has_been_downloaded(filename):\n",
    "    full_filename = \"../\" + filename\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/Fuenfgeld/DMA2022DataProjectC/main/\" + filename\n",
    "    if os.path.isfile(full_filename):\n",
    "        logger.log(\"File {} already exists, skipping download\".format(filename))\n",
    "    else:\n",
    "        logger.log(\"Downloading {}\".format(filename))\n",
    "        download_file(url, full_filename)\n",
    "\n",
    "def download_file(url, filename):\n",
    "    with open(filename, 'wb') as out_file:\n",
    "        with urlopen(url) as file:\n",
    "            out_file.write(file.read())\n",
    "\n",
    "if not os.path.isfile(\"extract.py\"):\n",
    "    download_file(\n",
    "        \"https://raw.githubusercontent.com/Fuenfgeld/DMA2022DataProjectC/main/src/extract.py\",\n",
    "        \"extract.py\"\n",
    "    )\n",
    "\n",
    "dataChanged = False\n",
    "for file in files:\n",
    "    for table in tables:\n",
    "        filename = file+table+\".csv\"\n",
    "        ensure_file_has_been_downloaded(filename)\n",
    "\n",
    "        with open(\"../\" + filename) as fileHandle:\n",
    "            fileContent = fileHandle.read()\n",
    "            fileHandle.close()\n",
    "\n",
    "        md5Hash = hashlib.md5(fileContent.encode()).hexdigest()\n",
    "        if md5Hashes[filename] != md5Hash:\n",
    "            dataChanged = True\n",
    "    \n",
    "if dataChanged:\n",
    "    logger.log(\"❌ Data set changed\")\n",
    "else:\n",
    "    logger.log(\"✅ Using original data set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mit Datenbank verbinden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databaseFile = \"data.sqlite\"\n",
    "\n",
    "logger.startTimeMeasurement('open-db', 'Connected to db and created tables')\n",
    "connection = extract.connect_to_db(logger, databaseFile)  # create table patients, observations, conditions, careplans\n",
    "logger.endTimeMeasurement('open-db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sqliteConnection(_logger):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tablesInDb = list(map(lambda tableResult: tableResult[0], cursor.fetchall()))\n",
    "    tablesInDb.sort()\n",
    "\n",
    "    for table in tables:\n",
    "        if not(table in tablesInDb):\n",
    "            raise Exception('Table not found:', table)\n",
    "\n",
    "testExecutor.execute('Test connection to database', test_sqliteConnection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten in Datenbank laden\n",
    "\n",
    "Lade der verwendete Daten in die Datenbank:\n",
    "\n",
    "-   careplans\n",
    "-   conditions\n",
    "-   observations\n",
    "-   patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.startTimeMeasurement('load-data', 'Loading data into db')\n",
    "for file in files:\n",
    "    for table in tables:\n",
    "        extract.insert_values_to_table(logger, connection.cursor(), table, \"../\"+ file + table + \".csv\")  # TODO: insert ALL values in the right tables \n",
    "        connection.commit()\n",
    "\n",
    "logger.endTimeMeasurement('load-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annonymisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "patientDf = pd.read_sql_query('SELECT * FROM patients;', connection)\n",
    "careplansDf = pd.read_sql_query('SELECT * FROM careplans;', connection)\n",
    "conditionsDf = pd.read_sql_query('SELECT * FROM conditions;', connection)\n",
    "observationsDf = pd.read_sql_query('SELECT * FROM observations;', connection)\n",
    "\n",
    "patientIds = [*patientDf.Id, *careplansDf.PATIENT, *conditionsDf.PATIENT, *observationsDf.PATIENT]\n",
    "\n",
    "def test_sanityCheckCombiningIds(_logger): \n",
    "    expectedLen = len(patientDf) + len(careplansDf) + len(conditionsDf) + len(observationsDf)\n",
    "    actualLen = len(patientIds)\n",
    "    if actualLen != expectedLen:\n",
    "        raise Exception('Not all patient ids were concatenated')\n",
    "\n",
    "testExecutor.execute('Sanity check: extracting all ids worked', test_sanityCheckCombiningIds)\n",
    "\n",
    "# Converts list to a set with only unique values\n",
    "uniqueIds = set(patientIds)\n",
    "logger.log(f\"{len(uniqueIds)} unique patient ids found\")\n",
    "if len(uniqueIds) >= len(patientDf.Id):\n",
    "    logger.log(f\"⚠️ The dataset contains {len(uniqueIds)} unique patientIds but only {len(patientDf.Id)} patients.\")\n",
    "\n",
    "annonymizedIds = {}\n",
    "for id in uniqueIds:\n",
    "    # Use uppercase here so it is easy to see if annonymized ids are used.\n",
    "    annonymizedIds[id] = hashlib.sha256(f\"{id}={random.random()}\".encode()).hexdigest().upper()\n",
    "\n",
    "def test_sanityEnsureAllIdsAreAnnonymized(_logger): \n",
    "    for id in patientIds:\n",
    "        if id in annonymizedIds:\n",
    "            raise Exception('A origin id still exists in anonnymized id list')\n",
    "\n",
    "testExecutor.execute('Sanity check: no origin ids exist anymore', test_sanityCheckCombiningIds)\n",
    "\n",
    "logger.startTimeMeasurement('annonymizedPatients', 'Writing annonymized patients')\n",
    "patientDf = patientDf.replace({\"Id\": annonymizedIds})\n",
    "patientDf.to_sql(name=\"anonnymized_patients\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('annonymizedPatients')\n",
    "\n",
    "logger.startTimeMeasurement('annonymizedCareplans', 'Writing annonymized careplans')\n",
    "careplansDf = careplansDf.replace({\"PATIENT\": annonymizedIds})\n",
    "careplansDf.to_sql(name=\"anonnymized_careplans\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('annonymizedCareplans')\n",
    "\n",
    "logger.startTimeMeasurement('annonymizedConditions', 'Writing annonymized conditions')\n",
    "conditionsDf = conditionsDf.replace({\"PATIENT\": annonymizedIds})\n",
    "conditionsDf.to_sql(name=\"anonnymized_conditions\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('annonymizedConditions')\n",
    "\n",
    "logger.startTimeMeasurement('annonymizedObservations', 'Writing annonymized Observations')\n",
    "observationsDf = observationsDf.replace({\"PATIENT\": annonymizedIds})\n",
    "observationsDf.to_sql(name=\"anonnymized_observations\", con=connection, if_exists='replace')\n",
    "logger.endTimeMeasurement('annonymizedObservations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messung der Datenfehler\n",
    "\n",
    "Für unsere Forschungsfrage sind nur alle Daten mit gemessenen BMI relevant. Wurde dieser nicht vermessen oder eingetragen können die Daten für die Forschungsfrage nicht verwendet werden und sind somit unbrauchbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL-Values\n",
    "\n",
    "Die Rohdaten werden zuvor auf die Anzahl an NULL-Values überprüft. Weisen mehr als **ein drittel der Daten**   Lücken in der Codierung auf, wird ein Fehler in der Verfassung angenommen und die Daten müssen manuell Überprüft werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counter = 0\n",
    "num_of_elements = 0\n",
    "for table in tables:\n",
    "    querie = f\"SELECT * from {table};\"\n",
    "    df = pd.read_sql_query(querie,connection)\n",
    "    result_string = str(df.isna().sum()).replace(\"\\n\",\" NULL-Values in Column \")\n",
    "    logger.log(f\"Found {result_string} null-values in {table}.\")\n",
    "    null_counter = df.isna().sum().sum() + null_counter\n",
    "    num_of_elements = num_of_elements + df.size\n",
    "perc_null_val = round(null_counter / num_of_elements,3)\n",
    "\n",
    "if perc_null_val > 0.33:\n",
    "    logger.log(f\"Found {perc_null_val} null-values.\",type='Warning')\n",
    "else:\n",
    "    logger.log(f\"Found {perc_null_val} null-values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prüfung auf Duplikate\n",
    "\n",
    "Duplikate verfälschen die  Ergebnisse des Anlyseteil durch Steigerung der Grundgesamheit mit gleichen Werten. Somit müssen die Daten auf Duplikate in den einzelnen Files überprüft werden, um gleiche Messungen zu finden und gegebenfalls im ETL-Process zu entfernen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_duplicates = 0\n",
    "num_of_elements = 0\n",
    "for table in tables:\n",
    "    querie = f\"SELECT * from {table};\"\n",
    "    df = pd.read_sql_query(querie,connection)\n",
    "    duplicates = df.groupby(df.columns.tolist()).size().reset_index().\\\n",
    "    rename(columns={0:'records'})\n",
    "    curr_num_duplicate = (duplicates.records -1).sum() \n",
    "    num_of_duplicates = num_of_duplicates + curr_num_duplicate\n",
    "    logger.log(f\"Found {curr_num_duplicate} duplicate-values in {table}.\")\n",
    "    num_of_elements = num_of_elements + df.size\n",
    "perc_duplicates = round(num_of_duplicates / num_of_elements,3)\n",
    "logger.log(f\"Found {perc_duplicates} duplicate-values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prozentuales Anzahl von Gewichts und BMI Werten für Patieten\n",
    "\n",
    "Für unsere Forschungsfrage sind BMI-Werte relevant und müssen für den Patienten mindestes einmal codiert worden sein. Um die Forschungsfrage mit den zur vorliegenden Daten zu beanworten, sollten auch hier mindestens **ein drittel der Daten** mit einen BMI Codiert worden sein.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patients_query = \"\"\"\n",
    "SELECT COUNT(id) FROM patients;\"\"\"\n",
    "count_bmi_query = \"\"\"\n",
    "SELECT COUNT(distinct id) FROM patients JOIN observations on patients.id == observations.patient WHERE observations.Code = '59576-9'\"\"\"\n",
    "\n",
    "count_all_bmi_query = f\"\"\"\n",
    "SELECT COUNT(patient) FROM observations WHERE observations.Code = '59576-9'\"\"\"\n",
    "\n",
    "patient_all_count = connection.execute(all_patients_query).fetchall()[0][0]\n",
    "patient_bmi_count = connection.execute(count_bmi_query).fetchall()[0][0]\n",
    "bmi_count = connection.execute(count_all_bmi_query).fetchall()[0][0]\n",
    "ratio = round(patient_bmi_count/patient_all_count, 3) \n",
    "\n",
    "logger.log(f\"Total num of patients {patient_all_count}.\")\n",
    "\n",
    "if ratio > 0.33:\n",
    "    logger.log(f\"Found {patient_bmi_count} patients ({round(ratio*100,3)}%) with {bmi_count} BMI-values.\")\n",
    "else:\n",
    "    logger.log(f\"Found {patient_bmi_count} patients ({round(ratio*100,3)}%) with {bmi_count} BMI-values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen Dimensionstabellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle patients_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table patients_\n",
    "cursor.execute('''DROP TABLE IF EXISTS patients_;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE patients_ ( \n",
    "        ID STRING PRIMARY KEY UNIQUE,\n",
    "        RACE STRING, \n",
    "        ETHNICITY STRING,\n",
    "        GENDER STRING,\n",
    "        AGE INT64\n",
    "        );''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df patients\n",
    "cursor.execute('''SELECT ID, BIRTHDATE, DEATHDATE, RACE, ETHNICITY, GENDER FROM PATIENTS;''')\n",
    "df_patients = pd.DataFrame(cursor.fetchall(), columns=['ID', 'BIRTHDATE', 'DEATHDATE', 'RACE', 'ETHNICITY', 'GENDER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to date\n",
    "df_patients[\"DEATHDATE\"] = pd.to_datetime(df_patients[\"DEATHDATE\"])\n",
    "df_patients[\"BIRTHDATE\"] = pd.to_datetime(df_patients[\"BIRTHDATE\"])\n",
    "# fill null values withh todays date\n",
    "df_patients['DEATHDATE'] = df_patients.DEATHDATE.fillna(pd.to_datetime(\"today\"))\n",
    "# calculate age\n",
    "df_patients[\"AGE\"] = df_patients.DEATHDATE.dt.year - df_patients.BIRTHDATE.dt.year\n",
    "# drop unnecessary variables\n",
    "df_patients = df_patients.drop(['BIRTHDATE', 'DEATHDATE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Duplicated Rows\", df_patients.duplicated(df_patients.columns).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients.to_sql('df_patients', connection, if_exists='replace', index=False)\n",
    "cursor.execute('INSERT INTO patients_ (ID, RACE, ETHNICITY, GENDER, AGE) SELECT ID, RACE, ETHNICITY, GENDER, RACE FROM df_patients;')\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_patients;''')\n",
    "\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(patients_)\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle observations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table observations_\n",
    "cursor.execute('''DROP TABLE IF EXISTS observations_;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE observations_ ( \n",
    "        CODE STRING PRIMARY KEY UNIQUE,\n",
    "        DESCRIPTION STRING,\n",
    "        UNITS STRING, \n",
    "        TYPE STRING\n",
    "        );''')\n",
    "\n",
    "# create df observation\n",
    "cursor.execute('''SELECT CODE, DESCRIPTION, UNITS, TYPE FROM OBSERVATIONS;''')\n",
    "df_observations = pd.DataFrame(cursor.fetchall(), columns=['CODE','DESCRIPTION', 'UNITS', 'TYPE'])\n",
    "\n",
    "df_observations = df_observations.drop_duplicates(subset='CODE')\n",
    "\n",
    "# transform dt in table\n",
    "df_observations.to_sql('df_observations', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('''INSERT INTO observations_ (CODE, DESCRIPTION, UNITS, TYPE) SELECT CODE, DESCRIPTION, UNITS, TYPE FROM df_observations;''')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_observations;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(observations_)\", connection))\n",
    "#print(pd.read_sql_query(\"SELECT * FROM observations_\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle careplans_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table careplans_code\n",
    "cursor.execute('''DROP TABLE IF EXISTS careplans_code;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE careplans_code ( \n",
    "        CODE STRING PRIMARY KEY UNIQUE,\n",
    "        DESCRIPTION STRING\n",
    "        );''')\n",
    "\n",
    "# create df careplans_code\n",
    "cursor.execute('''SELECT CODE, DESCRIPTION FROM CAREPLANS;''')\n",
    "df_careplans_code = pd.DataFrame(cursor.fetchall(), columns=['CODE','DESCRIPTION'])\n",
    "\n",
    "df_careplans_code = df_careplans_code.drop_duplicates(subset='CODE')\n",
    "\n",
    "# transform dt in table\n",
    "df_careplans_code.to_sql('df_careplans_code', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('''INSERT INTO careplans_code (CODE, DESCRIPTION) SELECT CODE, DESCRIPTION FROM df_careplans_code;''')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_careplans_code;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(careplans_code)\", connection))\n",
    "#print(pd.read_sql_query(\"SELECT * FROM careplans_code\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle careplans_reasoncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table careplans_reasoncode\n",
    "cursor.execute('''DROP TABLE IF EXISTS careplans_reasoncode;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE careplans_reasoncode ( \n",
    "        REASONCODE STRING PRIMARY KEY UNIQUE,\n",
    "        REASONDESCRIPTION STRING\n",
    "        );''')\n",
    "\n",
    "# create df careplans_code\n",
    "cursor.execute('''SELECT REASONCODE, REASONDESCRIPTION FROM CAREPLANS;''')\n",
    "df_careplans_reasoncode = pd.DataFrame(cursor.fetchall(), columns=['REASONCODE','REASONDESCRIPTION'])\n",
    "\n",
    "df_careplans_reasoncode = df_careplans_reasoncode.drop_duplicates(subset='REASONCODE')\n",
    "\n",
    "# transform dt in table\n",
    "df_careplans_reasoncode.to_sql('df_careplans_reasoncode', connection, if_exists='replace', index=False)\n",
    "   \n",
    "cursor.execute('INSERT INTO careplans_reasoncode (REASONCODE, REASONDESCRIPTION) SELECT REASONCODE, REASONDESCRIPTION FROM df_careplans_reasoncode;')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_careplans_reasoncode;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(careplans_reasoncode)\", connection))\n",
    "#print(pd.read_sql_query(\"SELECT * FROM careplans_reasoncode\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle conditions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table conditions\n",
    "cursor.execute('''DROP TABLE IF EXISTS conditions_;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE conditions_ ( \n",
    "        CODE STRING PRIMARY KEY UNIQUE,\n",
    "        DESCRIPTION STRING\n",
    "        );''')\n",
    "\n",
    "# create df conditions\n",
    "cursor.execute('''SELECT CODE, DESCRIPTION FROM CONDITIONS;''')\n",
    "df_conditions = pd.DataFrame(cursor.fetchall(), columns=['CODE','DESCRIPTION'])\n",
    "\n",
    "df_conditions = df_conditions.drop_duplicates(subset='CODE')\n",
    "\n",
    "# transform dt in table\n",
    "df_conditions.to_sql('df_conditions', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('INSERT INTO conditions_ (CODE, DESCRIPTION) SELECT CODE, DESCRIPTION FROM df_conditions;')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_conditions;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(conditions_)\", connection))\n",
    "#print(pd.read_sql_query(\"SELECT * FROM conditions_\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datumstabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def create_date_table(start='1900-01-01', end=datetime.today().strftime('%Y-%m-%d')):\n",
    "    \n",
    "    df_date = pd.DataFrame({\"Date\": pd.date_range(start, end)})\n",
    "\n",
    "    days_names = {\n",
    "        i: name\n",
    "        for i, name\n",
    "        in enumerate(['Monday', 'Tuesday', 'Wednesday',\n",
    "                      'Thursday', 'Friday', 'Saturday', \n",
    "                      'Sunday'])\n",
    "    }\n",
    "   \n",
    "    df_date[\"Day\"] = df_date.Date.dt.dayofweek.map(days_names.get)\n",
    "    df_date[\"Week\"] = df_date.Date.dt.weekofyear\n",
    "    df_date[\"Quarter\"] = df_date.Date.dt.quarter\n",
    "    df_date[\"Year\"] = df_date.Date.dt.year\n",
    "    df_date[\"Year_half\"] = (df_date.Quarter + 1) // 2\n",
    "    \n",
    "    return df_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table date_table\n",
    "cursor.execute('''DROP TABLE IF EXISTS date_table;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE date_table ( \n",
    "        DATE DATE PRIMARY KEY UNIQUE,\n",
    "        DAY STRING,\n",
    "        WEEK INT16,\n",
    "        QUARTER INT16,\n",
    "        YEAR INT16,\n",
    "        YEAR_HALF INT16\n",
    "        );''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = create_date_table()\n",
    "\n",
    "# transform dt in table\n",
    "df_date.to_sql('df_date', connection, if_exists='replace', index=False)\n",
    "\n",
    "cursor.execute('INSERT INTO date_table (DATE, DAY, WEEK, QUARTER, YEAR, YEAR_HALF) SELECT DATE, DAY, WEEK, QUARTER, YEAR, YEAR_HALF FROM df_date;')\n",
    "\n",
    "cursor.execute('''DROP TABLE IF EXISTS df_date;''')\n",
    "print(pd.read_sql_query(\"PRAGMA table_info(date_table)\", connection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstelle Faktentabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('''DROP TABLE IF EXISTS fact_table;''')\n",
    "cursor.execute('''\n",
    "        CREATE TABLE fact_table ( \n",
    "        PATIENT_ID STRING,\n",
    "        OBSERVATION_CODE STRING,\n",
    "        VALUE STRING,\n",
    "        DATE DATE,\n",
    "        CONDITIONS_CODE STRING,\n",
    "        ENDDATE DATE,\n",
    "        CAREPLANS_CODE STRING,\n",
    "        CAREPLANS_REASONCODE STRING,\n",
    "        FOREIGN KEY (PATIENT_ID)\n",
    "            REFERENCES patients_(ID),\n",
    "        FOREIGN KEY (OBSERVATION_CODE)\n",
    "            REFERENCES observations_(CODE),\n",
    "        FOREIGN KEY (CONDITIONS_CODE)\n",
    "            REFERENCES conditions_(CODE),\n",
    "        FOREIGN KEY (CAREPLANS_CODE)\n",
    "            REFERENCES careplans_code(CODE),\n",
    "        FOREIGN KEY (CAREPLANS_REASONCODE)\n",
    "            REFERENCES careplans_reasoncode(REASONCODE)\n",
    "        );''')\n",
    "\n",
    "cursor.execute('''INSERT INTO fact_table        \n",
    "                    (PATIENT_ID, OBSERVATION_CODE, VALUE, DATE) \n",
    "                    SELECT PATIENT, CODE, VALUE, DATE \n",
    "                    FROM OBSERVATIONS\n",
    "                    ;''')\n",
    "\n",
    "cursor.execute('''INSERT INTO fact_table        \n",
    "                    (PATIENT_ID, CAREPLANS_CODE, DATE, ENDDATE) \n",
    "                    SELECT PATIENT, CODE, START, STOP \n",
    "                    FROM CAREPLANS\n",
    "                    ;''')\n",
    "\n",
    "cursor.execute('''INSERT INTO fact_table        \n",
    "                    (PATIENT_ID, CAREPLANS_REASONCODE, DATE, ENDDATE) \n",
    "                    SELECT PATIENT, REASONCODE, START, STOP \n",
    "                    FROM CAREPLANS\n",
    "                    ;''')\n",
    "\n",
    "cursor.execute('''INSERT INTO fact_table        \n",
    "                    (PATIENT_ID, CONDITIONS_CODE, DATE, ENDDATE) \n",
    "                    SELECT PATIENT, CODE, START, STOP \n",
    "                    FROM CONDITIONS\n",
    "                    ;''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.read_sql_query(\"PRAGMA foreign_key_list(fact_table)\", connection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('SELECT name FROM sqlite_master where type=\"table\"')\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Show the the percentile of a patient's body mass index indicates the relative position of the patient's BMI number among the given population. For children and teens the interpretation of BMI is both age- and sex-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df_bmi_perc = pd.read_sql(\"SELECT * FROM fact_table WHERE observation_code = '59576-9'\",connection)\n",
    "df_bmi_perc['VALUE'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bmi = pd.read_sql(\"SELECT * FROM fact_table WHERE observation_code ='39156-5'\",connection)\n",
    "plt.hist(df_bmi['VALUE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_height_weight = pd.read_sql(\"SELECT distinct fact_table.patient_id, fact_table.value,weight.value FROM fact_table left JOIN (SELECT distinct patient_ID,value FROM fact_table WHERE observation_code = '29463-7') AS weight on weight.patient_ID==fact_table.patient_id WHERE observation_code = '8302-2' \",connection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix \n",
    "df_height_weight = pd.read_sql(\"SELECT distinct fact_table.patient_id, fact_table.value,weight.value FROM fact_table left JOIN (SELECT distinct patient_ID,value FROM fact_table WHERE observation_code = '29463-7') AS weight on weight.patient_ID==fact_table.patient_id WHERE observation_code = '8302-2' \",connection)\n",
    "df_height_weight = df_height_weight.rename({'VALUE': 'height', 'value': 'weight'}, axis=1)\n",
    "plt.scatter(df_height_weight.height,df_height_weight.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con = pd.read_sql(\"SELECT conditions.patient,conditions.code,conditions.description,observations.value\\\n",
    " FROM conditions join observations on observations.patient==conditions.patient WHERE observations.Code = '39156-5' AND conditions.description NOT LIKE '%finding%'\",connection)\n",
    "plt.pie(df_con['DESCRIPTION'].value_counts()[:10],labels=df_con['DESCRIPTION'].value_counts()[:10].index)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verteilung Oberservation/Conditions\n",
    "\n",
    "Verschiedener Beobachtungen zu den Top 5 Krankheitesaufkommen in unseren Datensatz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_five = pd.DataFrame(df_con['DESCRIPTION'].value_counts()[:5]).reset_index()\n",
    "top_five = top_five.rename({'index': 'DESCRIPTION', 'DESCRIPTION': 'COUNT'}, axis=1)\n",
    "top_five['CODE'] = df_con['CODE'].value_counts()[:5].index\n",
    "top_five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 8302-2,Body Height\n",
    "- 29463-7,Body Weight\n",
    "- 39156-5,Body Mass Index\n",
    "- 8462-4,Diastolic Blood Pressure\n",
    "- 8480-6,Systolic Blood Pressure\n",
    "- 8867-4,Heart rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_labels = ['Viral sinusitis (disorder)','Acute viral pharyngitis (disorder)','Hypertension','Acute bronchitis (disorder)','Prediabetes']\n",
    "disease_codes = [444814009,195662009,59621000,10509002,15777000]\n",
    "observation_lables = ['Body Height','Body Weight','Body Mass Index','Diastolic Blood Pressure','Systolic Blood Pressure','Heart rate']\n",
    "observation_codes = ['8302-2','29463-7','39156-5','8462-4','8480-6','8867-4']\n",
    "df_lists = list()\n",
    "for dis_code,dis_lable in zip(disease_codes,disease_labels):\n",
    "    df = pd.DataFrame()\n",
    "    for ob_code,ob_lable in zip(observation_codes,observation_lables):\n",
    "        sql_pattern = f\"\"\"\n",
    "        SELECT observations.date,conditions.patient,conditions.code,observations.value\n",
    "            FROM conditions join observations on observations.patient==conditions.patient\n",
    "            WHERE observations.Code = '{ob_code}' AND conditions.code = '{dis_code}'\"\"\"\n",
    "        df[ob_lable] = pd.read_sql(sql_pattern,connection)['VALUE']\n",
    "    df['Disease'] = dis_lable\n",
    "    df_lists.append(df)\n",
    "df_data = pd.concat(df_lists).reset_index(drop=True)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['class'] = ''\n",
    "df_data.loc[df_data['Body Mass Index'].values<18,'class'] = 'underweight'\n",
    "df_data.loc[df_data['Body Mass Index'].values>=18,'class'] = 'normal'\n",
    "df_data.loc[df_data['Body Mass Index'].values>=25,'class'] = 'overweight'\n",
    "df_data.loc[df_data['Body Mass Index'].values>=30,'class'] = 'obese'\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.boxplot(x=\"Body Mass Index\", y=\"Disease\", data=df_data)\n",
    "plt.axvline(x=25, c='red',ls=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.ecdfplot(data=df_data, x=\"Body Mass Index\",hue='Disease')\n",
    "plt.axvline(x=25, c='red',ls=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hypertension = df_data.where(df_data['Disease']== 'Hypertension')\n",
    "sns.barplot(x=df_hypertension['class'].value_counts().index,y=df_hypertension['class'].value_counts().values)\n",
    "plt.title(\"Hypertension\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Regression](https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html#sphx-glr-auto-examples-cross-decomposition-plot-pcr-vs-pls-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df_reg = df_data.dropna()\n",
    "X, y = df_reg.drop(['Disease','class'],axis=1)['Body Mass Index'].values.reshape((-1, 1)), df_reg['Disease']\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs').fit(X,y)\n",
    "\n",
    "model.predict(np.array([25,]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Decision tree](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "df_tree = df_data.dropna()\n",
    "X, y = df_tree.drop(['Disease','class'],axis=1), df_tree['Disease']\n",
    "clf = tree.DecisionTreeClassifier(max_depth=5,max_features='sqrt',random_state=42)\n",
    "clf = clf.fit(X, y)\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# need to munch time to run in CI\n",
    "plot_tree(clf, filled=True)\n",
    "plt.savefig('../images/decision_tree.png',dpi=800)\n",
    "plt.title(\"Decision tree\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufräumen & Logs speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()\n",
    "logger.logTimings()\n",
    "logger.writeToFile(\"../artefacts-for-release/analysis-log.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
